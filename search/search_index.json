{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>addonfactory-ucc-test (AUT) is an open-source testing framework for functional tests for UCC-based Splunk Add-ons which allows to test add-ons functonality for data ingestion. It automates add-ons configuration, events generation by vendor product side and assessment of ingested events providing platform for end to end tests.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Prepared basic setup for the add-on<ul> <li>Vendor product configured for the add-on</li> <li>Splunk instance with add-on installed</li> <li>The setup is manually tested</li> </ul> </li> <li>openapi.json saved to developer workstation</li> <li>docker installed and started</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>addonfactory-ucc-test can be installed via pip from PyPI: <pre><code>pip install splunk-add-on-ucc-modinput-test\n</code></pre> You can verify installation by checking installed version: <pre><code>ucc-test-modinput --version\n</code></pre></p>"},{"location":"#how-can-i-run-existing-tests","title":"How can I run existing tests?","text":"<p>If you just want to run existing functional tests developed with use of the framework:</p> <ol> <li> <p>make sure the prerequisites are met and addonfactory-ucc-test is installed</p> </li> <li> <p>export environment variables that describe your Splunk instance and the one specific for add-on you want to test (they should be described in <code>ucc_modinput_functional/README.md</code>)</p> </li> <li> <p>run <code>ucc-test-modinput gen</code></p> </li> <li> <p>run <code>pytest tests/ucc_modinput_functional/</code></p> </li> </ol> <p></p>"},{"location":"#writing-all-tests-from-scratch","title":"Writing all tests from scratch","text":"<p>Building a comprehensive test suite requires careful planning and adherence to best practices. Following paragraphs outlines the key aspects of starting from scratch, including the Design Principles that guide test architecture, the Test Scenarios that ensure coverage, important considerations Before You Write Your First Line of Code, and best practices When You Write Your Tests.</p> <p>That is a lot to read.</p> <p>You want to start small and simple?</p> <p>Check our Hello World example first.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#204-2025-10-21","title":"2.0.4 (2025-10-21)","text":""},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>splunk cloud identification (#149) (688b1d3)</li> </ul>"},{"location":"CHANGELOG/#203-2025-10-21","title":"2.0.3 (2025-10-21)","text":""},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>splunk cloud identification (#148) (e22e777)</li> </ul>"},{"location":"CHANGELOG/#202-2025-10-17","title":"2.0.2 (2025-10-17)","text":""},{"location":"CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>increase backoff delay for index creation retries on 424/503 errors (#147) (6e03f0d)</li> </ul>"},{"location":"CHANGELOG/#201-2025-08-25","title":"2.0.1 (2025-08-25)","text":""},{"location":"CHANGELOG/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>update index creation for azure cloud instance (#144) (d67d22d)</li> </ul>"},{"location":"CHANGELOG/#200-2025-08-08","title":"2.0.0 (2025-08-08)","text":""},{"location":"CHANGELOG/#features","title":"Features","text":"<ul> <li>add support for creating metric index (c9f9e00)</li> <li>add support for creating metric index (#134) (38cfd58)</li> <li>release 2.0.0 (#135) (8516563)</li> <li>search for all types of indexes in get_index (35ef4b5)</li> <li>support kvstore api (#131) (930a076)</li> </ul>"},{"location":"CHANGELOG/#breaking-changes","title":"BREAKING CHANGES","text":"<ul> <li>change get_index method interface so it\u2019s able to get indexes from idm stacks.</li> </ul>"},{"location":"CHANGELOG/#pr-type","title":"PR Type","text":"<p>What kind of change does this PR introduce? * [X] Feature * [ ] Bug Fix * [ ] Refactoring (no functional or API changes) * [X] Documentation Update * [ ] Maintenance (dependency updates, CI, etc.)</p>"},{"location":"CHANGELOG/#summary","title":"Summary","text":"<p>Test runs:</p> <p>https://cd.splunkdev.com/taautomation/ta-automation-compatibility-tests/-/pipelines/28335229</p> <p>https://cd.splunkdev.com/taautomation/ta-automation-compatibility-tests/-/pipelines/28340518</p> <p>https://cd.splunkdev.com/taautomation/ta-automation-compatibility-tests/-/pipelines/28533993</p> <p>https://cd.splunkdev.com/taautomation/ta-automation-compatibility-tests/-/pipelines/28533938</p>"},{"location":"CHANGELOG/#changes","title":"Changes","text":"<p>Please provide a summary of the changes.</p>"},{"location":"CHANGELOG/#user-experience","title":"User experience","text":"<p>Please describe the user experience before and after this change. Screenshots are welcome for additional context.</p>"},{"location":"CHANGELOG/#checklist","title":"Checklist","text":"<p>If an item doesn\u2019t apply to your changes, leave it unchecked.</p> <ul> <li> I have performed a self-review of this change according to the development guidelines</li> <li> Tests have been added/modified to cover the changes (testing doc)</li> <li> Changes are documented</li> <li> PR title and description follows the contributing principles</li> </ul>"},{"location":"CHANGELOG/#101-2025-07-02","title":"1.0.1 (2025-07-02)","text":""},{"location":"CHANGELOG/#bug-fixes_4","title":"Bug Fixes","text":"<ul> <li>add error handling while tests collection (#108) (263b900)</li> <li>set proper attribute name for token (#120) (ea0eb1d)</li> </ul>"},{"location":"CHANGELOG/#100-2025-05-27","title":"1.0.0 (2025-05-27)","text":""},{"location":"addonfactory-ucc-test_pytest_plugin/","title":"addonfactory-ucc-test pytest plugin","text":"<p>addonfactory-ucc-test plugin extendst pytest to support end-to-end Splunk add-ons flow.</p> <p>It comes with libraries that cover standard Splunk (Enterprise and Cloud) functionalities as well as general UCC functionalities, such as:</p> <ul> <li> <p>indexes creation</p> </li> <li> <p>searching</p> </li> <li> <p>common configuration and inputs management</p> </li> </ul> <p>Relevant environment variables have to be defined to benefit from the functionalities.</p> <p>The plugin requires vendor product specific and add-on specific functionalities to be covered by add-on developer. This includes specifying environment variables that include information about vendor product address, user that should be used to generate events or user that should be used for integration.</p>"},{"location":"addonfactory-ucc-test_pytest_plugin/#expected-environment-variables","title":"Expected environment variables","text":"<p>Information about Splunk has to be given in relevant environment variables:</p> <ul> <li> <p>MODINPUT_TEST_SPLUNK_HOST - Splunk Enterprise IP or domain address or Splunk Cloud domain address (eg. <code>127.0.0.1</code>, <code>localhost</code> or <code>test.splunkcloud.com</code>). <code>https</code> protocol is used for connection and ssl verification is skipped to support developer and test Splunk instances.</p> </li> <li> <p>MODINPUT_TEST_SPLUNK_PORT - management port (<code>8089</code> in most cases)</p> </li> <li> <p>MODINPUT_TEST_SPLUNK_USERNAME - Splunk admin username (<code>admin</code> in most cases) that will be used for tests</p> </li> <li> <p>MODINPUT_TEST_SPLUNK_PASSWORD_BASE64 - base64 encoded Splunk user used for tests password  (eg. <code>Q2hhbmczZCE=</code>  as a result of <code>ucc-test-modinput base64encode -s 'Chang3d!'</code>; check ucc-test-modinput documentation for more)</p> </li> <li> <p>(optional) MODINPUT_TEST_SPLUNK_DEDICATED_INDEX - existing index name that should be used to write test events. If not defined, dedicated index is created for each test run and used for the same purpose.</p> </li> <li> <p>following variables are required only if Splunk Cloud is used for tests and any index needs to be created for tests:</p> <ul> <li> <p>MODINPUT_TEST_SPLUNK_TOKEN_BASE64 - base64 encoded Splunk Cloud authentication token</p> </li> <li> <p>MODINPUT_TEST_ACS_SERVER -  ACS server (eg. <code>https://admin.splunk.com</code>, <code>https://staging.admin.splunk.com</code> or <code>https://admin.splunkcloudgc.com</code>)</p> </li> <li> <p>MODINPUT_TEST_ACS_STACK - ACS stack, that in majority of the cases will be just part of Splunk Cloud domain address (eg. for <code>ucc-test.stg.splunkcloud.com</code>, ACS server would be <code>https://staging.admin.splunk.com</code> and ACS stack <code>ucc-test</code>)</p> </li> </ul> </li> </ul>"},{"location":"addonfactory-ucc-test_pytest_plugin/#action-diagram","title":"Action diagram","text":""},{"location":"addonfactory-ucc-test_pytest_plugin/#plugin-arguments","title":"Plugin arguments","text":"<p>addonfactory-ucc-test pytest plugin comes with following arguments:</p> <ul> <li> <p><code>--sequential-execution</code> - use no threading (for debugging)</p> </li> <li> <p><code>--do-not-fail-with-teardown</code> - do not fail test if test\u2019s teardown fails. By default a test will fail if any of its forges teardowns fail, even if the test itself passed.</p> </li> <li> <p><code>--do-not-delete-at-teardown</code> - do not delete created resoueces at teardown. This flag is for debug purposes and should be handled by developer if needed. For example, based on this flag developers can add alternative code to forges\u2019 teardowns, to disable inputs instead of deleting them in order to study inputs configurations after tests execution.</p> </li> <li> <p><code>--number-of-threads=[NUMBER_OF_THREADS]</code> - number of threads to use to execute forges. Allowed range: [10, 20]. Default value: 10.</p> </li> <li> <p><code>--probe-invoke-interval=[PROBE_INVOKE_INTERVAL]</code> - interval in seconds used to repeat invocation of yes/no type of probes. Allowed range: [1, 60]. Default value: 5.</p> </li> <li> <p><code>--probe-wait-timeout=[PROBE_WAIT_TIMEOUT]</code> - maximum time in seconds given to a single probe to turn positive. Allowed range: [60, 600]. Default value: 300.</p> </li> <li> <p><code>--bootstrap-wait-timeout=[BOOTSTRAP_WAIT_TIMEOUT]</code> - maximum time in seconds given to all bootstrap tasks to finish. Allowed range: [300, 3600]. Default value: 1800.</p> </li> <li> <p><code>--attached-tasks-wait-timeout=[ATTACHED_TASKS_WAIT_TIMEOUT]</code> - maximum time in seconds given to finish all tasks attached to a single test. Allowed range: [60, 1200]. Default value: 600.</p> </li> <li> <p><code>--completion-check-frequency=[COMPLETION_CHECK_FREQUENCY]</code> - frequency to check that bootstrap or attached tasks bundle has finished to execute. Allowed range: [1, 30]. Default value: 5.</p> </li> </ul>"},{"location":"before_you_write_your_first_line_of_code/","title":"Before you write your first line of code","text":"<p>AUT is a powerful toolset.</p> <p>Add-on developer experience is the most important for us and we don\u2019t want you to get lost in what is available for you.</p>"},{"location":"before_you_write_your_first_line_of_code/#learn-from-splunk-add-on-for-example","title":"Learn from Splunk Add-on for Example","text":"<p>Before you start working on your own tests, check splunk-example-ta to get basic understanding of the example TA. Think how you would test it.</p> <p>Open <code>tests/ucc_modinput_functional</code> and go through it in proposed below order to see how it is tested. Are your ideas addressed?</p>"},{"location":"before_you_write_your_first_line_of_code/#testsucc_modinput_functional","title":"tests/ucc_modinput_functional","text":"<ol> <li> <p><code>README.md</code> - contains add-on specific information related to the functional tests</p> </li> <li> <p><code>defaults.py</code> - contains predefined, the tests-specific, constant values</p> </li> <li> <p><code>vendor/</code> - contains vendor product-specific code</p> <ol> <li> <p><code>configuration.py</code> - to read configuration from environment variables; it can be used later for vendor product-specific means (eg. triggering action that would generate event available for add-on to collect), add-on configuration or within test functions</p> </li> <li> <p><code>client.py</code> - contains code used to communicate with vendor product</p> </li> </ol> </li> <li> <p><code>splunk/</code> - contains add-on specific code</p> <ol> <li> <p><code>client.py</code> - contains code used to communicate with add-on REST API; relevant code snippets can be found in swagger_client README.md copied from there, pasted to the client file and adopted</p> </li> <li> <p><code>forges.py</code> - contains functions responsible for creation and deletion of resources and configurations (forges); <code>yield</code> in each forge, separates setup and teardown</p> </li> <li> <p><code>probes.py</code> - contains functions validating specific conditions used to make sure that execution of a specific forge indeed resulted in creation of expected resource.</p> </li> </ol> </li> <li> <p><code>test_configuration.py</code> - start simple, eg. from making sure the simplest case like <code>test_ta_logging</code> works fine. Keep adding following tests for add-on configuration to make sure you are able to define building blocks that will be used for inputs</p> </li> <li> <p><code>test_inputs.py</code> - you have proper configuration. There are still two things you need to confirm:</p> <ol> <li> <p>Make sure vendor product is talkative enough to have always events available for your tests or you need to trigger events generation</p> </li> <li> <p>Input forge should return spl query you will use in input probe as well as in test to get raw events for assertion</p> </li> </ol> </li> </ol>"},{"location":"before_you_write_your_first_line_of_code/#also-worth-considering","title":"\u2026 also worth considering","text":"<p>There are components you may still want to add to your tests:</p> <ol> <li> <p><code>vendor/</code> </p> <ol> <li> <p><code>forges.py</code> - use if you want to setup and teardown resources in vendor product</p> </li> <li> <p><code>probes.py</code></p> </li> </ol> </li> <li> <p><code>splunk/</code></p> <ol> <li><code>configuration.py</code> - this file is to cover values not related to vendor product, such as proxy accounts</li> </ol> </li> <li> <p><code>test_proxies.py</code> - to test proxies. Proxy configuration is general for specific add-on, so if defined it will be used for all configuration entries as well as inputs. When constructing this kind of tests you want to isolate them that can be achieved by using <code>attach</code> decorator that would group following tasks:</p> <ol> <li> <p>making sure proxy is defined as required (or disabled if we want to test without proxy configured)</p> </li> <li> <p>relevant configuration creation - especially if validation is used that requires relevant connection to vendor product</p> </li> <li> <p>input creation</p> </li> <li> <p>etc.</p> </li> </ol> </li> <li> <p><code>test_validators.py</code> - to test that add-on will not accept improper values for its configuration.</p> </li> <li> <p>etc.</p> </li> </ol> <p>Above is just a proposition that may be relevant for small to medium add-ons.</p> <p>If you find your add-on more complex, feel free to organize the test structure the way you find the most convenient and efficient.</p>"},{"location":"before_you_write_your_first_line_of_code/#ucc-test-modinput-init","title":"ucc-test-modinput init","text":"<p>Init command is created to save some of your efforts by doing boilerplate actions: </p> <ul> <li>generates swagger client supporting modules, </li> <li>creates unified tests file structure, </li> <li>bootstraps basic splunk and vendor clients together with configuration classes, </li> <li>initial tests with forges and probes required for them.</li> </ul> <p>This command should be executed once before before any unified tests are created for the project.</p> <ol> <li> <p>Before invoking init command, please, make sure:</p> <ol> <li> <p>the prerequsities are met</p> </li> <li> <p>addonfactory-ucc-test is installed</p> </li> </ol> </li> <li> <p>Run <code>init</code> to have following directories generated for you: <pre><code>ucc-test-modinput init\n</code></pre></p> <ol> <li> <p><code>swagger_client</code> directory with supporting modules</p> </li> <li> <p><code>tests/ucc_modinput_functional</code> directory with relevant files and some UCC related tests.</p> </li> </ol> </li> </ol> <p>Note: You may want to specify openapi.json file location - eg. if it is in <code>Downloads</code>: <pre><code>ucc-test-modinput init --openapi-json ~/Downloads/openapi.json\n</code></pre> Visit <code>ucc-test-modinput</code> page for more</p> <ol> <li> <p>Set environment variables for your Splunk instance. Eg.: <pre><code>export MODINPUT_TEST_SPLUNK_HOST=localhost\nexport MODINPUT_TEST_SPLUNK_PORT=8089\nexport MODINPUT_TEST_SPLUNK_USERNAME=admin\nexport MODINPUT_TEST_SPLUNK_PASSWORD_BASE64=$(ucc-test-modinput base64encode -s 'Chang3d!')\n</code></pre></p> </li> <li> <p>Run the tests <pre><code>pytest tests/ucc_modinput_functional\n</code></pre></p> </li> </ol> <p>Note: If your add-on code contains customisations for out of the box components (such as logging or proxy), some tests may fail.</p> <p></p>"},{"location":"contributing/","title":"Contributing Guidelines","text":"<p>We welcome contributions from the community! This guide will help you understand our contribution process and requirements.</p>"},{"location":"contributing/#development-guidelines","title":"Development guidelines","text":"<ol> <li>Small PRs (blogpost)</li> <li>When fixing a bug, include a test that reproduces the issue in the same pull request (the test should fail without your changes)</li> <li>If you are refactoring, ensure adequate test coverage exists for the target area. If coverage is insufficient, create tests in a separate pull request first. This approach provides a safety net for validating current behavior and simplifies code reviews.</li> </ol>"},{"location":"contributing/#build-and-test","title":"Build and Test","text":"<p>Prerequisites:</p> <ul> <li>Poetry 1.5.1. Installation guide</li> </ul> <p>Build a new local version:</p> <pre><code>poetry build\n</code></pre>"},{"location":"contributing/#unit-tests","title":"Unit tests","text":"<pre><code>poetry run pytest tests/unit\n</code></pre>"},{"location":"contributing/#linting-and-type-checking","title":"Linting and Type-checking","text":"<p><code>addonfactory-ucc-test</code> uses the <code>pre-commit</code> framework for linting and type-checking. Consult with <code>pre-commit</code> documentation about what is the best way to install the software.</p> <p>To run it locally:</p> <pre><code>poetry run pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#documentation-changes","title":"Documentation changes","text":"<p>Documentation changes are also welcome!</p> <p>To verify changes locally:</p> <pre><code>poetry run mkdocs serve -a localhost:8001\n</code></pre>"},{"location":"contributing/#issues-and-bug-reports","title":"Issues and bug reports","text":"<p>If you\u2019re seeing some unexpected behavior with AUT, create an issue on GitHub. You can click on \u201cNew Issue\u201d and use the template provided.</p>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<p>We love to see pull requests!</p>"},{"location":"contributing/#pr-title","title":"PR Title","text":"<p>We follow Conventional Commits for PR titles. The title format is crucial as we squash commits during merge, and this PR title will be used in the release notes (for feat and fix types). Here\u2019s a short TL;DR of the format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\nTypes:\n- feat: New feature (user facing)\n- fix: Bug fix (user facing)\n- docs: Documentation changes (user facing)\n- style: Code style changes (formatting, etc.)\n- refactor: Code changes that neither fix bugs nor add features\n- perf: Performance improvements\n- test: Adding or updating tests\n- chore: Maintenance tasks\n</code></pre> <p>Example: <code>feat(ui): add new input validation for text fields</code></p>"},{"location":"contributing/#pr-description","title":"PR Description","text":"<p>Includes:</p> <ul> <li>Motivation behind the changes (any reference to issues or user stories)</li> <li>High level description of code changes</li> <li>Description of changes in user experience if applicable.</li> <li>Steps to reproduce the issue or test the new feature, if possible. This will speed up the review process.</li> </ul> <p>After submitting your PR, GitHub will automatically add relevant reviewers, and CI checks will run automatically.</p> <p>Note: <code>semgrep</code> and <code>fossa</code> checks might fail for external contributors. This is expected and will be handled by maintainers.</p>"},{"location":"contributing/#release-flow","title":"Release flow","text":"<p>The instructions below utilize the GitHub CLI tool, which you can install via HomeBrew:</p> <pre><code>brew install gh\ngh auth login\n</code></pre> <ul> <li>The default development branch is <code>develop</code>. Use this branch for creating pull requests (PRs) for your features, fixes, documentation updates, etc. PRs to the <code>develop</code> branch should be merged using the squash option on GitHub.</li> <li>When it\u2019s time for a release (handled by the UCC team), create a PR from <code>develop</code> to <code>main</code> using the following commands:</li> </ul> <pre><code>gh pr create --title \"chore: merge develop into main\" --body \"\" --head develop --base main\n# set automerge with merge commit to avoid accidentally squashing PR\ngh pr merge develop --auto --merge\n</code></pre> <ul> <li>Ensure CI passes and await team review.</li> <li>PR should be merged using merge commit option in GitHub (already included in the command)</li> <li>Releases are made automatically (both on GitHub and PyPI), and a bot will push a commit to <code>main</code> with all necessary changes</li> <li>If necessary, update release notes and CHANGELOG.md accordingly to the content of the release.</li> <li>If any issue was solved by this release, remove waiting-for-release label from it and then close the issue.</li> <li>After the release, backport the bot\u2019s changes to the <code>develop</code> branch:</li> </ul> <pre><code>gh pr create --title \"chore: merge main into develop\" --body \"\" --head main --base develop\n# set automerge with merge commit to avoid accidentally squashing PR\ngh pr merge main --auto --merge\n</code></pre> <ul> <li> <p>If a release encounters issues requiring a quick bug fix (handled by the AUT team):</p> <ul> <li>Create a PR to the main branch with the fix, including tests that reproduce and then fix the issue.</li> <li>Ensure CI passes and await team review.</li> <li>Merge the PR using the merge commit option on GitHub.</li> <li>Backport the bug fix PR to the develop branch.</li> </ul> </li> <li> <p>After release is done, announce it to community on slack channels:</p> <ul> <li>Internal UCC test channel</li> <li>Splunk Usergroup UCC channel</li> </ul> </li> </ul>"},{"location":"design_principles/","title":"Design principles","text":"<p>The addonfactory-ucc-test framework follows principles in an order based on importance:</p> <ol> <li> <p>add-on developer experience</p> </li> <li> <p>execution time</p> </li> <li> <p>test scenarios complexity</p> </li> <li> <p>data isolation</p> </li> <li> <p>cross-platform &amp; CI-ready</p> </li> <li> <p>framework deepdive</p> </li> </ol>"},{"location":"design_principles/#building-blocks","title":"Building blocks","text":"<p>The addonfactory-ucc-test framework consists of following building blocks:</p> <ul> <li> <p>addonfactory-ucc-test that contains:</p> <ul> <li> <p><code>ucc-test-modinput</code> CLI tool used to initialise the tests (creates relevant directories, files and initial test; one time action), generate add-on SDK and other supporting actions (text encryption and decryption) </p> </li> <li> <p><code>addonfactory-ucc-test/functional</code> pytest plugin used to extend pytest functionality to support end-to-end functional tests </p> </li> </ul> </li> <li> <p>supporting artifacts:</p> <ul> <li> <p><code>ucc_modinput_functional</code> tests in <code>Splunk Add-on for Example</code> </p> </li> <li> <p>this documentation</p> </li> </ul> </li> </ul>"},{"location":"design_principles/#concepts-to-use-and-rules-to-follow","title":"Concepts to use and rules to follow","text":"<p>Framework comes with libraries used to deal with Splunk (Enterprise as well as Cloud), UCC-related functionalities and common actions.</p> <p>There are following concepts used in the framework as well as rules add-on developer should follow:</p> <ol> <li> <p>Vendor product-related and add-on specific functionalities are left to the developer to deal with</p> </li> <li> <p>test functions should be used just to assert actual vs expected values</p> </li> <li> <p>test functions are wrapped by forge decorators that define setup and teardown tasks</p> </li> <li> <p>forge can yield <code>Dict[str,Any]</code>. Key becomes globally available variable that refers to relevant value</p> </li> <li> <p>probe function can be defined for forge to optimise setup time</p> </li> <li> <p>forge functions are executed in a sequence as they appear  - that means setup tasks are executed in a sequence of appearance while tear down tasks are executed in reversed order</p> </li> <li> <p>forges decorator allows to group forge tasks that can be executed parallely</p> </li> <li> <p>bootstrap decorators group forge tasks that are common for many tests</p> </li> <li> <p>attach decorators group forge tasks that are specific for certain test</p> </li> </ol> <p>Note: Order of importance is discussed separately.</p>"},{"location":"design_principles/#performance","title":"Performance","text":"<ul> <li> <p>bootstrap makes sure setup and teardown tasks are executed just once, no matter for how many tests they are required.</p> </li> <li> <p>probes when applied for setup tasks, makes setup task is finished as soon as expected state is achieved.</p> </li> <li> <p>forges allows to parallelise independent tasks.</p> </li> </ul>"},{"location":"design_principles/#complexity","title":"Complexity","text":"<p>The framework is thought the way, to be able to address even the most complicated Splunk add-ons. To achieve this goal, each forge should cover just one functionality. This way it becomes atomic. Atomic components can be connected, related or group flexible.</p>"},{"location":"design_principles/#data-isolation","title":"Data isolation","text":"<p>There are certain ways data can be isolated:</p> <ul> <li> <p>dedicated index is created for each test run by default and it is highly recommended to use the index. Moreover, AUT provides a functionality that allows to create custom indexes if needed</p> </li> <li> <p>attach decorator allows to isolate specific tests so time range can be defined for splunk events</p> </li> <li> <p>source of the event allows to identify input</p> </li> <li> <p>unique test id can be used to distinguish between specific tests and test runs</p> </li> </ul>"},{"location":"design_principles/#supported-platforms","title":"Supported platforms","text":"<p>This framework is supported on the most popular workstations (MacOS, Linux, Windows) as well as CIs (GitHub, GitLab).</p>"},{"location":"framework_deepdive/","title":"Framework deep dive","text":"<p>Not yet finished, writing is still in progress \u2026</p>"},{"location":"framework_deepdive/#framework-philosophy","title":"Framework philosophy","text":"<p>This section explains the background why this framework was created, what it tries to solve and what ideas it implements</p>"},{"location":"framework_deepdive/#popular-add-on-test-scenarios","title":"Popular add-on test scenarios","text":"<p>Spunk technical add-on is an application which goal is to interact with vendor customer environment in order to pull required vendor specific data and then send it to Splunk environment to be ingested and saved in desired Splunk indexes. This makes functional testing of add-ons to stick to a couple of common test scenarios:</p> <ol> <li>Test of Add-on configuration validators: </li> </ol> <ul> <li> <p>interact with add-on API endpoint trying to configure various add-on supported objects (inputs and supporting configuration files) using incorrect values</p> </li> <li> <p>expecting that corresponding API endpoint rejects these values with expected explanatory error message in response and proper error logged.</p> </li> <li> <p>Delete configuration and objects created for test from Splunk instance.</p> </li> </ul> <p>For example, test can try to configure add-on proxy configuration using unsupported port number and expect that proxy will not be configured and the endpoint responses with an error message clearly pointing to unsupported port number.</p> <ol> <li>Data ingestion validation:</li> </ol> <ul> <li> <p>interact with customer environment to apply configurations, create objects or reproduce user actions in order to recreate conditions making desired vendor specific data to appear and ready for pulling to Splunk.</p> </li> <li> <p>interact with add-on at Splunk instance to apply configurations and create objects in order engage required add-on modular input to ingest data created for it in customer environment.</p> </li> <li> <p>validate ingestion by interacting with Splunk services to makes sure that expected number of events of expected type have been ingested as well as make sure that add-on modular input did not log any error messages during ingestion process.</p> </li> <li> <p>Delete configuration and objects created for test from customer environment and Splunk instance.</p> </li> </ul> <p>For example, functional test may verify correct ingestion of data from AWS S3 bucket which may require to configure S3 bucket and upload a data file prepared for test, configure corresponding input the way it would point to correct resource at created S3 bucket, let this input to run and ingest expected data, execute Splunk searches to confirm the number and sourcetypes of ingested events are correct, execute Splunk searches to make sure input did not log any error messages during ingestion process.</p>"},{"location":"framework_deepdive/#test-requirements","title":"Test requirements","text":"<p>Based on the test scenarios and examples listed above it is clear that for each add-on functional test before actual verification developer should deal with preparing test requirements, i.e. creation of necessary resources at Splunk or vendor related environment - objects and configurations, possibly uploading some test data. Next step is to retrieve some of test environment state properties and only then do the actual checks by comparing retrieved parameters values with expectations. Another thing to take care is to remove prepared resources after the test completion or at the end of test session. There are even more topics to think about connected with test execution optimization: for example, if several test can safely reuse the same resource it would be better to preserve this resource to let it be reused instead of creating and deleting it several times for each tests; or what if several resources for a tests are independent then it would be faster to create them in parallel. Taking care about such improvements makes tests better but at the same time adds complexity and may make them less clear and straightforward and less maintainable. Unified functional test framework was developed with desire make it take care about all the mentioned improvements while hiding from developers all related complexity and giving clear and straightforward way to create test and describe test requirements. </p>"},{"location":"framework_deepdive/#framework-requirements","title":"Framework requirements","text":"<p>To address the challenges listed above, several core requirements have been developed to be implemented by the framework:</p> <ol> <li> <p>Framework should provide a declaration way of describing test requirements (resources required for tests) outside the test function body.</p> </li> <li> <p>Test requirements should be provided in same area as the tests function they belong to and give clear understanding what resources are needed, and in what order they should be created. </p> </li> <li> <p>Test itself should be as small as possible and only contain as much code as needed for verifications (assert statements) and optionally the code to collect data needed for verifications.</p> </li> <li> <p>Dependencies between resources should be declared as a flat list in contrast to recursive approach where code creating resource also creates all resources it depends on which increases code complexity and hides dependencies from developer potentially making the code difficult to understand and support.</p> </li> <li> <p>Developer should be able to specify if some resources can be created in parallel.</p> </li> <li> <p>Developer should be able to specify what resources can be created in advance and what should be created right before test execution.</p> </li> <li> <p>Framework should understand lifetime of a resource and execute code removing the resource as soon as all tests using it are executed. Developer should have a way to alter default framework behavior for specific tests.</p> </li> <li> <p>Developer should be able to specify conditions for certain resources that can be used by framework to confirm that this resource is created or to wait for this resource to be created during required time.</p> </li> </ol>"},{"location":"framework_deepdive/#framework-basic-concepts","title":"Framework basic concepts","text":"<p>To address the above requirements framework introduces a concept of forges - reusable functions responsible for creation and optionally deletion of required resources. They are implemented by developer separately from tests and bound to test functions using \u2018bootstrap\u2019 or \u2018attach\u2019 decorators. The order in which forges are listed within decorators defines the dependencies between forges. In other words, forge declared earlier becomes a prerequisite for a forge declared later. As a result framework will make sure that dependent forge is executed after its dependency. If two or more forges do not depend on each other, they can be declared in the same \u201cline\u201d or block of forges which would tell framework to execute these forges in parallel if possible. </p> <p>Forges can accept data via function arguments. The values for those arguments can be defined by a user explicitly or via parametrized arguments when declaring a forge as test requirement. Another way for a forge to receive argument values is to collect them from test artifactory. Artifactory is an internal storage of key-value pairs maintained for each test function separately. The mapping of values from artifactory to forge arguments is done by framework automatically by arguments names. Forges can update artifactory values or create new ones through returned or yielded values. </p> <p>When forge function is assigned to a specific tests it will receive argument values specific for this test that may be the same or differ from other test. Depending on argument values the results of forge executions also can be the same or different. In some sense one can think about assigned forge as about a <code>task</code> that creates a resource or fulfills an action. Framework assumes that forge functions is not affected by any any side values or processes, which means that executing the same forge function with the same argument values should results in the same action taken or in the same resource created. Based on this, same forge executed twice with different argument values is treated by framework as two different <code>tasks</code> while when executed with the same argument values will be treated as the same <code>task</code> creating the same resource and by default framework will try to skip unnecessary executions. Internally framework operates with tasks entities that are combinations of a forge function with specific argument values and optionally attached probe and  assigned to a specific test, however developer does not deal with tasks objects directly. </p> <p>When assigning a forge to a test, one of two additional properties that developer can configuration is \u201cprobe\u201d. Probe property allows to attach a probe function that will be used by framework to make sure that action taken by forge indeed has expected result, i.e. a resource has been created or a specific configuration has been applied. Framework will be invoking probe function with certain interval until probe gets successful or time given for probe to success expires. Just like forges probe functions can access test artifactory variables via declared function arguments. Last boolean value returned by a probe will be added by framework to the test artifactory with the name of probe function as the property name so the test function will be able to verify if the probe was successful or exited because of timeout.</p> <p>Scope is the second of the two arguments that forge can accept when assigned to a tests. Before forge function execution framework pays attention to forge function name, forge function arguments values and forge scope. If there several tests that have the same forge with the same scope assigned to them and those forges have the same call argument values, framework assumes it is the same forge (or forge represents the same task) dedicated to create the same resource and executes it only once. By default all forges have \u201csession\u201d scope which means this comparison will take place across all the executed tests.  Changing scope value at forge assignment to tests allows to narrow the scope of tests for which forge execution results may be reused, for example to all tests in the same module or to a single test. Note that as soon as the last test using forge is executed, framework invokes teardown code of the forge if such code was implemented by developer.</p> <p>It is also important which of the two decorators are used to assign forge to a test. Bootstrap decorator assumes that forge should create resource before executing tests. All forges assigned to different tests taken as independent and executed in parallel - first go all forges at bootstrap list top, then all in the second place and so on. Attach decorator works differently - it invokes forges right before the test execution. Bootstrap is more preferable way to execute forges to achieve better tests execution times, but it requires from developer more efforts to make sure that forges of different tests do not compete for configuring the same resource. There are some cases where it\u2019s possible to avoid this competition, for example when dealing with global or single instance resources like testing different proxy configurations. In those cases it\u2019s important to apply each specific configuration right before execution of (i.e. specifically for) related test function and \u2018attach\u2019 decorator is the proper way to do this.</p> <p>It is important to mention that framework also allows tests to receive (or subscribe for) test artifactory properties generated during forge executions just by using their names to test function argument list. This way forges and probes can prepare some useful values in test artifactory and let test to use them for necessary verifications. </p> <p>In some sense forges and pytest fixtures have a lot in common, however they are very different in one important way - the way of organizing dependencies between them. To make a fixture A to depend on results of fixture B, fixture B should be declared as argument of fixture A.  Implemented this way, the relationship between fixture A and fixture B are hardcoded and B cannot be replaced with another fixture that would generate expected output using different source data or algorithm. When fixture A is used in tests, its dependencies are hidden from developer, and to understand what fixture A does, developer should study its dependency B as well, and inside be he may discover other dependencies and so on. In big test projects relationship between fixtures can become pretty sophisticated. Forges dependencies, in contrast, are dynamic and rely on artifactory variables provided by dependency forges, that allows to recombine forges according to desired test scenario and taking into account arguments expected by the forge function and generated by it artifacts. When talking about declaring test dependencies, they all (test dependencies together with forges dependencies) are declared in a form of a flat list located right before test function that gives to developer a clear picture about test requirements, i.e. resources to be created for the test execution.</p>"},{"location":"framework_deepdive/#framework-structure-and-entities","title":"Framework structure and entities","text":"<p>Framework is created as pytest plugin and relays on pytest mechanisms of tests execution. Above pytest functionality framework collects information about required forges, tries to execute them optimally respecting defined dependencies and making sure that corresponding tests are not executed before all declared by developer requirements are met.</p> <p></p>"},{"location":"framework_deepdive/#splunk-client","title":"Splunk client","text":"<p>Splunk client is a combination of two classes - client class, responsible for interactions with Splunk environment, and configuration class, responsible for providing necessary settings for client class to act and some test data and settings for forge, probe and test functions. This separation is chosen to make it possible to apply different configuration to the same client class and let it communicate with different Splunk instances or point to different test samples and so on. Using provided client and configuration classes framework creates a separate instance of client class for each test and make it available to forge, probe and test functions via internally supported function arguments.</p>"},{"location":"framework_deepdive/#splunk-client-class","title":"Splunk client class","text":"<p>Splunk client class is supposed to implement methods providing access to Splunk and add-on specific API. This client class is what developer deals with through framework supported function argument <code>splunk_client</code> when implementing framework entities like forges, probes and tests themselves. Developer can use either default client class available out of the box or extend it with add-on API support as well as with custom methods to add support for Splunk related functionality not supported by the framework. Framework tries to save developers\u2019 efforts by automating the creation of splunk client class when developer executes framework <code>init</code> and <code>gen</code> commands. These commands use openapi.json file generated by UCC while building TA package to create swagger support classes for add-on API endpoints. Then, based on swagger classes, framework generates a new managed client class inherited from framework base client class. At the same time framework generates one more \u201cdeveloper facing\u201d client class inherited from the managed client class. By doing this framework adds support for add-on API and also prepares area for developers\u2019 custom code extensions.</p>"},{"location":"framework_deepdive/#splunk-functionality-supported-out-of-the-box","title":"Splunk functionality supported out of the box.","text":"<ul> <li> <p>splunk_client.instance_epoch_time() - returns current time at splunk instance in epoch format</p> </li> <li> <p>splunk_client.search(query) - executes SPL search query at Splunk instance and returns result in SearchState class object (splunk_add_on_ucc_modinput_test.common.splunk_instance.SearchState)</p> </li> <li> <p>splunk_client.create_index(name, datatype) - creates Splunk index with a given name and data type (event or metric). Returns splunklib.Index object.</p> </li> <li> <p>splunk_client.default_index() - returns default Splunk index name if framework configured to create one.</p> </li> <li> <p>splunk_client.search_probe(probe_spl, verify_fn, timeout, interval, probe_name) - probe generator function to simplify creation of framework probes based on Splunk searches.</p> </li> <li> <p>splunk_client.repeat_search_until(spl, condition_fn, timeout, interval) - methods execution Splunk search continuously in defined intervals until it gets expected result or reaches timeout.</p> </li> <li> <p>splunk_client.instance_file_helper() - Factory method for SplunkInstanceFileHelper, allowing to execute some file operations directly on Splunk host. Requires <code>Splunk Add-on for Modinput Test</code> to be installed on the Splunk host. May be used for creation and verification of file based checkpoints. </p> </li> <li> <p>splunk_client.app_file_helper() - same as instance_file_helper except SplunkInstanceFileHelper (splunk_add_on_ucc_modinput_test.functional.common.splunk_instance_file) will treat provided file paths as relative to add-on root folder on Splunk host. Requires <code>app_name</code> property to be part of <code>splunk_client</code> configuration class.</p> </li> <li> <p>splunk_client.kvstore_api_helper() - Factory method for SplunkInstanceKVStoreAPI, allowing to execute api calls for KV store directly on Splunk host. Requires <code>app_name</code> and <code>app_user</code> properties to be part of splunk_client configuration class. It is used to get the record from KV store collection. </p> </li> </ul>"},{"location":"framework_deepdive/#add-on-api-endpoint-support","title":"Add-on API endpoint support","text":"<p>Each technical add-on creates an additional set of API endpoints responsible for add-on resources like inputs, configuration files and custom rest handlers.  As mentioned earlier framework <code>init</code> and <code>gen</code> commands </p>"},{"location":"framework_deepdive/#client-configuration-class","title":"Client configuration class","text":"<p>Client configuration class is a separate class dedicated to collect all settings for client class. Framework contains implementation of the base configuration class to collect settings for base client class functionality. Developer has an option to extend it by inheriting a new class form base configuration class and adding new properties as well as redefine values or sources of default properties. Framework <code>init</code> command generates default client configuration class that looks like the following <pre><code>from splunk_add_on_ucc_modinput_test.functional.splunk import (\n    SplunkConfigurationBase,\n)\nclass Configuration(SplunkConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        # to be implemented\n        # self.encoded_prop = utils.get_from_environment_variable(\n        #     \"ENV_PROP_NAME1\", string_function=utils.Base64.decode\n        # )\n        # self.not_encoded_prop = utils.get_from_environment_variable(\n        #     \"ENV_PROP_NAME2\"\n        # )\n        pass\n</code></pre> As seen from the example, developer has to overwrite <code>customize_configuration</code> method to add additional configuration properties. This method is called from class __init__ method and added to simplify defining of new configuration properties by avoiding overwriting  __init__ method itself that requires some specific arguments expected by framework. </p> <p>Collection of default framework properties is implemented through class methods listed below. If there is a need to alter values, sources or collection algorithms for some of these properties, corresponding methods should be overwritten.</p> <ul> <li> <p><code>collect_host(cls) -&gt; str | None</code> - to collect Splunk host name. Default value source is environment variable MODINPUT_TEST_SPLUNK_HOST.</p> </li> <li> <p><code>collect_port(cls) -&gt; str | None</code> - to collect Splunk host port. Default value source is environment variable MODINPUT_TEST_SPLUNK_PORT.</p> </li> <li> <p><code>collect_username(cls) -&gt; str | None</code> - to collect Splunk user name. Default value source is environment variable MODINPUT_TEST_SPLUNK_USERNAME.</p> </li> <li> <p><code>collect_password(cls) -&gt; str | None</code> - to collect Splunk user password. Default value source is environment variable MODINPUT_TEST_SPLUNK_PASSWORD_BASE64.</p> </li> <li> <p><code>collect_splunk_dedicated_index(cls) -&gt; str | None</code> - to collect name of existing Splunk index. Default value source is environment variable MODINPUT_TEST_SPLUNK_DEDICATED_INDEX.</p> </li> <li> <p><code>collect_splunk_token(cls, is_optional: bool) -&gt; str | None</code> - to collect Splunk token. Default value source is environment variable MODINPUT_TEST_SPLUNK_TOKEN_BASE64.</p> </li> <li> <p><code>collect_acs_server(cls, is_optional: bool) -&gt; str | None</code> - to collect Splunk ACS service url. Default value source is environment variable MODINPUT_TEST_ACS_SERVER.</p> </li> <li> <p><code>collect_acs_stack(cls, is_optional: bool) -&gt; str | None</code> - to collect Splunk CS stack. Default value source is environment variable MODINPUT_TEST_ACS_STACK.</p> </li> </ul> <p>Note that <code>is_optional</code> is boolean argument tells method if property is treated as optional or mandatory. If property is mandatory and method fails to collect value it should log a critical error and raise SplunkClientConfigurationException exception (splunk_add_on_ucc_modinput_test.common.utils).  <pre><code>class Configuration(SplunkConfigurationBase):\n    ...\n    @classmethod\n    def collect_host(cls) -&gt; str | None:\n        return &lt;your code to provide the value&gt;\n\n    @classmethod\n    def collect_acs_server(cls, is_optional: bool) -&gt; str | None:\n        try:\n            return &lt;your code to provide the value&gt;\n        except Exception:\n            if is_optional:\n                return None\n            else:\n                logger.critical(\"Your error message\")\n                raise SplunkClientConfigurationException()\n</code></pre> Beside default framework properties, configuration class gives access to command argument values defined for pytest executions via properties: <code>probe_invoke_interval</code>, <code>probe_wait_timeout</code>, <code>do_not_fail_with_teardown</code> and so on. It\u2019s easy to guess any property name for any argument by removing from argument name leading dashes and replacing internal dashes with underscores, for example <code>--bootstrap-wait-timeout</code> argument name turns to <code>bootstrap_wait_timeout</code> property name.</p>"},{"location":"framework_deepdive/#splunk-client-and-configuration-classes-binding-decorators","title":"Splunk Client and configuration classes binding decorators","text":"<p>When custom Splunk client and configuration classes are implemented there is one more step to be done to let framework know about these classes and use them. This can be done by using one of the decorators: <code>register_splunk_class</code> or <code>define_splunk_client_argument</code>. </p>"},{"location":"framework_deepdive/#register_splunk_classswagger_client-splunk_configuration_class","title":"register_splunk_class(swagger_client, splunk_configuration_class)","text":"<p>This decorator should be applied to the Splunk client class created by a developer. It takes imported swagger client module as the first argument and binds it to client. Second argument is Splunk client configuration class implemented by developer. Decorator registers it in framework together with the Splunk client class. Usage of this decorator may look as the following <pre><code>import swagger_client\n\nclass Configuration(SplunkConfigurationBase):\n    # your configuration class implementation\n\n@register_splunk_class(swagger_client, Configuration)\nclass SplunkClient(ManagedSplunkClient):\n    # code extending base Splunk client class\n</code></pre> Having both classes registered, framework is capable to create instances of Splunk client classes with configuration class assigned to the instance and make it available via splunk_client builtin function argument in forges, probes and tests.  <pre><code>def my_forge(splunk_client: SplunkClient):\n    splunk_client.some_splunk_client_method() # some splunk client class method (exact name depends on the class implementation)\n</code></pre></p> <p>The idea behind implementing client and configuration classes separately is to make it possible to use Splunk client class with different configurations. This may be useful to give tests access to several Splunk instances, for example, to execute comparative verifications of different Splunk and/or add-on versions. For this framework has another decorator that allows to register Splunk client class multiply times with different configuration classes.</p>"},{"location":"framework_deepdive/#define_splunk_client_argumentswagger_client-splunk_client_class-splunk_class_argument_name","title":"define_splunk_client_argument(swagger_client, splunk_client_class, splunk_class_argument_name)","text":"<p>In contrast to <code>register_splunk_class</code>, this decorator must be applied to a configuration class. It takes imported swagger client module as the first argument and binds it to client class specified in the second decorator argument. Last decorator argument is optional and allows to define new builtin framework argument name for the pair of client and configuration classes. By default this argument has value \u2018splunk_client\u2019 so leaving it unspecified allows to overwrite binding to default builtin argument - when used this way both decorators work identically. However, defining different value will create a new builtin variable for specified combination of splunk client and configuration classes. Note that <code>splunk_class_argument_name</code> value when specified should comply with python rules of variable naming. <pre><code>import swagger_client\n\nclass SplunkClient(ManagedSplunkClient):\n    # code extending base Splunk client class\n\n@splunk_class_argument_name(swagger_client, SplunkClient)\nclass ConfigurationSplunk9(SplunkConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        # define configuration to access Splunk v9 instance\n\n@splunk_class_argument_name(swagger_client, SplunkClient, \"splunk_client_v10\")\nclass ConfigurationSplunk10(SplunkConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        # define configuration to access Splunk v10 instance\n</code></pre> In the example above <code>ConfigurationSplunk9</code> is registered without <code>splunk_class_argument_name</code> value specified, which means that framework will attach it and the client class to default <code>splunk_client</code> variable. For configuration class <code>ConfigurationSplunk10</code> <code>splunk_class_argument_name</code> is defined as \u201csplunk_client_v10\u201d which adds to framework new builtin variable splunk_client_v10 and through it framework will make available a Splunk client class object created using <code>ConfigurationSplunk10</code> configuration. This way forges, probes and tests will be able to use both builtin variables if needed: <pre><code>def my_forge1(splunk_client: SplunkClient, splunk_client_v10: SplunkClient):\n    splunk_client.some_splunk_client_method() # action at Splunk v9 instance\n    splunk_client_v10.some_splunk_client_method() # same action at Splunk v10 instance\n\ndef my_forge2(splunk_client_v10: SplunkClient):\n    splunk_client_v10.some_other_splunk_client_method() # another action at Splunk v10 instance\n</code></pre></p> <p>Note that if all client classes are registered with (bound to) custom internal variable names, internal variable \u2018splunk_client\u2019 will still be available and bound to default client and configuration classes with default methods and configuration properties defined.</p>"},{"location":"framework_deepdive/#vendor-client","title":"Vendor client","text":"<p>Vendor client is a combination of two classes - client class, responsible for interactions with vendor environment, and configuration class, responsible for providing necessary settings for client class to act and some test data and settings for forge, probe and test functions. This separation is chosen to make it possible to apply different configuration to the same client class and let it communicate with different vendor hosts or point to different test samples and so on. Using provided client and configuration classes framework creates a separate instance of client class for each test and make it available to forges, probes and tests via internally supported function arguments.</p>"},{"location":"framework_deepdive/#vendor-client-class","title":"Vendor client class","text":"<p>Vendor client class should be created by developer to access vendor related environments like vendor cloud services, appliances or even user desktop monitored by vendor tools in order to trigger desired events. Similar to Splunk client class, this client class is what developer deals with through framework supported function argument <code>vendor_client</code> when implementing framework entities like forges, probes and tests themselves. Framework creates one instance of this class object per test. Having no knowledge about possible vendor environments framework has very little out of the box support for vendor class: base classes for vendor client and configuration. The approach remains similar to the one used for Splunk classes - both custom vendor client and configuration classes must be implemented from base classes offered by framework and then registered in the framework using <code>register_vendor_class</code> or <code>define_vendor_client_argument</code> decorators.</p>"},{"location":"framework_deepdive/#configuration-class","title":"Configuration class","text":"<p>Similar to Splunk client configuration, Vendor client configuration class should be created to provide client class with required configuration. Framework binds client and configuration classes at runtime when creates client instances - one for each tests. Framework implements default vendor configuration class that provides access to the same set of command prompt arguments values specified for pytest execution. The only place to setup configuration properties is  <code>customize_configuration</code> method.  In contrast to Splunk client configuration class, it does not have any default vendor configuration properties, so there are no corresponding collection methods to overwrite.  <pre><code>from splunk_add_on_ucc_modinput_test.functional.vendor import (\n    VendorConfigurationBase,\n)\nclass Configuration(VendorConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        # to be implemented\n        # self.encoded_prop = utils.get_from_environment_variable(\n        #     \"ENV_PROP_NAME1\", string_function=utils.Base64.decode\n        # )\n        # self.not_encoded_prop = utils.get_from_environment_variable(\n        #     \"ENV_PROP_NAME2\"\n        # )\n        pass\n</code></pre></p>"},{"location":"framework_deepdive/#client-classes-register-decorators","title":"Client classes register decorators","text":"<p>When custom vendor client and configuration classes are implemented there is one more step to be done to let framework know about these classes and use them. Similar to Splunk classes, this can be done by using one of the decorators: <code>register_vendor_class</code> or <code>define_vendor_client_argument</code>. They act the same way as Splunk client binding decorators except they do not require swagger module to be specified.  <pre><code>class Configuration(VendorConfigurationBase):\n    # your configuration class implementation\n\n@register_vendor_class(Configuration)\nclass VendorClient(VendorClientBase):\n    # code extending base vendor client class\n</code></pre> If tests suppose to communicate with different vendor appliances based on different API, framework supports having several different vendor classes bound to different configurations and assigned to different framework internal variable names. Scenario with same class bound to different configurations is also supported. <pre><code>class CiscoMeraki(VendorClientBase):\n    # code extending base vendor client class\n\nclass CiscoWSA(VendorClientBase):\n    # code extending base vendor client class\n\n@vendor_class_argument_name(CiscoMeraki)\nclass Meraki132Configuration(VendorConfigurationBase, \"meraki132_client\"):\n    def customize_configuration(self) -&gt; None:\n        # define configuration to access Cisco Meraki v1.32.0 appliance\n\n@vendor_class_argument_name(CiscoMeraki)\nclass Meraki138Configuration(VendorConfigurationBase, \"meraki138_client\"):\n    def customize_configuration(self) -&gt; None:\n        # define configuration to access Cisco Meraki v1.38.0 appliance\n\n@vendor_class_argument_name(CiscoWSA, \"wsa_client\")\nclass WSAConfiguration(VendorConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        # define configuration to access Cisco WSA appliance\n</code></pre> In the examples above <code>register_vendor_class</code>binds <code>Configuration</code> class to <code>VendorClient</code> class and makes client instances created by this pair available through <code>vendor_client</code> variable. Another example demonstrates <code>vendor_class_argument_name</code> decorator binding same client class <code>CiscoMeraki</code> to two different configurations classes<code>Meraki132Configuration</code> and <code>Meraki138Configuration</code> that become available via <code>meraki132_client</code> and <code>meraki138_client</code> internal variables. As well as another binding of vendor client class <code>CiscoWSA</code> to it\u2019s own configuration class <code>WSAConfiguration</code> thad becomes available via <code>wsa_client</code> internal variable. This way forges, probes and tests will be able to use all four  builtin variables if needed: <pre><code>def my_forge1(meraki132_client: CiscoMeraki, meraki132_client: CiscoMeraki):\n    meraki132_client.some_meraki_client_method() # action at Meraki appliance with v1.32.0\n    meraki138_client.some_meraki_client_method() # same action at Meraki appliance with v1.32.0\n\ndef my_forge2(wsa_client: CiscoWSA):\n    wsa_client.some_ciscowsa_client_method() # another action at Cisco WSA appliance\n</code></pre> Note that iff all client classes registered with (bound to) custom internal variable name, internal variable \u2018vendor_client\u2019 will still be available and bound to default client and configuration classes with nor methods of configuration properties defined.</p>"},{"location":"framework_deepdive/#forges","title":"Forges","text":"<p>As said earlier, forges are reusable functions responsible for creation and optionally deletion of resources required by tests. Like pytest fixtures forges can receive and return values and can be implemented as regular or generator function functions. If implemented as a generator function, the first yield will be separating setup and teardown code of the forge. In the following sections these topics explained in more details:</p>"},{"location":"framework_deepdive/#forge-function-arguments","title":"Forge function arguments","text":"<p>Forge function can receive any number of arguments. Before forge is executed, framework analyses argument names and tries to collect and provide values for forge execution by mapping its function argument names to different internal dictionaries like test artifactory, built in arguments created by framework and arguments explicitly specified by user at forge assignment stage.</p>"},{"location":"framework_deepdive/#builtin-arguments-reserved-argument-names","title":"Builtin arguments (reserved argument names)","text":"<p>Framework supports the following out of the box builtin properties that can be mapped by name to forge, probe and test function arguments:</p> <ul> <li> <p>splunk_client - is an instance of splunk client class created by developer separately and registered in framework using corresponding decorator. Framework creates dedicated Splunk client class instance per test, initializing it using configuration class responsible for collecting necessary setting from different sources like environment variables, hardcoded values. There is a way to tell framework to create additional splunk client instance with different configuration that would be mapped it to desired function argument names, for example <code>splunk_v10_client</code>. This may be useful when running tests at two or more splunk instances at the same time, for example, for Splunk or add-on upgrade tests. </p> </li> <li> <p>vendor_client - similarly to splunk client class this one is an instance of vendor client class created by developer separately and registered in framework using corresponding decorator. Framework creates dedicated vendor client class instance per test, initializing it using configuration class responsible for collecting necessary setting from different sources like environment variables, hardcoded values. There is a way to tell framework to create additional vendor client instances with different configurations that would be mapped it to a desired function arguments names, for example vendor_client_appliance2.  This may be useful when running tests at two or more vendor instances at the same time, for example, when testing with two different vendor appliances or using instances running different versions of vendor software.</p> </li> <li> <p>session_id - is a unique identifier generated by framework for each test execution. It may be helpful to name reused resources to make sure that from test execution to execution those resources have unique names</p> </li> <li> <p>test_id - is a unique identifier generated by framework for each test during tests execution. It may be helpful to name resources dedicated to specific tests to avoid conflicts between different tests that other way may by chance get their resources named identically.</p> </li> </ul>"},{"location":"framework_deepdive/#forges-as-regular-functions","title":"Forges as regular functions","text":"<p>Below is example of a forge implemented as a regular function <pre><code>def my_forge(splunk_client: SplunkClient, test_id: str, other_argument: str):\n    input_name = f\"my_input_{test_id}\"\n    splunk_client.create_some_input(input_name, other_argument) # some splunk client class method to create an input (exact name depends on the add-on)\n    return input_name\n</code></pre> When execution above forge for specific test framework will first collect all necessary forge arguments, i.e. splunk_client and test_id will be taken from builtin arguments, other_argument will be mapped either from test artifactory, explicitly defined arguments or from parametrized arguments. Next framework will execute forge with prepared arguments passing them to forge function as keyword arguments. When forge is executed framework gets its return value. If return value is not None framework will create new new key-value pair (artefact) in test artifacts with the name of forge function as key and returned value as a value. </p> <p>There is a way to let forge to control the name of created artifact as well as to tell framework to save several artifacts. For this instead of returning a single value forge can return a dictionary object with desired artifact names mapped to desired returning values. For example <pre><code>def my_forge(splunk_client: SplunkClient, test_id: str, other_argument: str):\n    input_name = f\"my_input_{test_id}\"\n    successful = splunk_client.create_some_input(input_name, other_argument)  # some splunk client class method to create an input\n    return dict(  # it's recommended to use dict() constructor to makes sure that artifact name used is a valid python variable.\n        input_name = input_name,\n        successful = successful\n    )\n</code></pre> Above forge function returns dictionary telling framework to update artifactory with \u2018input_name\u2019 and \u2018successful\u2019 artifacts with corresponding values. Note, that if artifacts with the same names already exist the will be overwritten.</p>"},{"location":"framework_deepdive/#forges-as-generators-functions","title":"Forges as generators functions","text":"<p>Forges as generators are useful when resources created by forge need to be removed and forge should contain teardown code to fullfil deletion. In that case yield statement of generator function splits setup and teardown code like in the following example: <pre><code>def my_forge(splunk_client: SplunkClient, test_id: str, other_argument: str):\n    input_name = f\"my_input_{test_id}\"\n    successful = splunk_client.create_some_input(input_name, other_argument) # some splunk client class method to create an input\n    yield dict(  # it's recommended to use dict() constructor to makes sure that artifact name used is a valid python variable.\n        input_name = input_name,\n        successful = successful\n    )\n    # teardown code starts here\n    if successful:\n        splunk_client.delete_some_input(input_name)\n</code></pre> As seen from the example, now instead of returning values to be stored in artifactory they should be yielded. It is fine to use yield without any value - this will mean that forge does not intend to update or create any artifacts in test artifactory. Note that as soon as forge invokes yield operator function return value will be ignored ignored. However if forge function has yield statement but does not yield, framework will again rely on returned forge value. This allows framework to support scenarios with conditional teardown. In other words, when forge needs teardown code to be executed it yields, and if teardown is not needed it returns. <pre><code>def my_forge(splunk_client: SplunkClient, test_id: str, does_not_need_teardown: bool = False):\n    input_name = f\"my_input_{test_id}\"\n    successful = splunk_client.create_some_input(input_name, other_argument) # some splunk client class method to create an input\n    artifacts = dict(  # it's recommended to use dict() constructor to makes sure that artifact name used is a valid python variable.\n        input_name = input_name,\n        successful = successful\n    )\n    if does_not_need_teardown:\n        return artifacts\n\n    yield artifacts:\n\n    # teardown code starts here\n    if successful:\n        splunk_client.delete_some_input(input_name)\n</code></pre> In above example forge skips (turns off) teardown block by using <code>return</code> statement when function argument <code>does_not_need_teardown</code> value is True.</p>"},{"location":"framework_deepdive/#artifactory","title":"Artifactory","text":"<p>Artifactory is an internal storage of key-value pairs maintained for each test function separately. It stores variables added by framework based on analysis of values provided by forges and probes. Test artifactories maintained by the framework automatically based on results collected from forges and probes. As well framework handles mapping of artifacts to forge, probe and test function arguments. This means that as soon as a new key value pair is added to test artifactory it can be used by forge, probe and test functions just by declaring function arguments using names of stored artifacts. For example, let\u2019s have a forge that creates an S3 bucket at AWS environment and returns <code>bucket_name</code> artefact <pre><code>def create_s3_bucket(vendor_client: VendorClient, test_id: str, bucket_config: Dict[str, object]):\n    bucket_name = f\"my_s3_bucket_{test_id}\"\n    vendor_client.create_s3_bucket(bucket_name, bucket_config) # some vendor client class method to create an s3 bucket\n    return dict(bucket_name=bucket_name)\n</code></pre> Now we can create another forge that would upload files with required data samples to created s3 bucket and returns another artifact - <code>successfully_uploaded</code>: <pre><code>def upload_s3_bucket_files(vendor_client: VendorClient, bucket_name: str, data_file_path: str):\n    bucket_name = f\"my_s3_bucket_{test_id}\"\n    success = vendor_client.upload_file_to_s3_bucket(bucket_name, data_file_path) # some vendor client class method to upload files to s3 bucket\n    return dict(successfully_uploaded=success)\n</code></pre> And finally we implement a test function that depends on the above two forges: <pre><code>@bootstrap(\n    forge(create_s3_bucket, bucket_config={\"some\": \"config\"}),\n    forge(upload_s3_bucket_files, data_file_path=\"some/file/path\")\n)\ntest_s3_bucket(vendor_client: VendorClient, bucket_name: str, successfully_uploaded: bool):\n    assert list_of_bucket_resources is True\n    list_of_bucket_resources = vendor_client.list_bucket_files(bucket_name)\n    assert expected_resource_name in list_of_bucket_resources\n</code></pre> Above demonstrates how <code>bucket_name</code> and <code>successfully_uploaded</code> artifacts returned by forges can be used in arguments of other forges and in test function arguments. <code>bootstrap</code> decorator and other control elements required to declare test dependencies are explained in next section. Note where values for other forge arguments are taken from: - <code>vendor_client</code> and <code>test_id</code> are builtin arguments provided by the framework.  - <code>bucket_config</code> and <code>data_file_path</code> are defined explicitly by developer at test requirements declaration section.</p>"},{"location":"framework_deepdive/#forge-assignment-scope","title":"Forge assignment <code>scope</code>","text":"<p>Forge assignment <code>scope</code> is a name of a common name or a marker grouping several forge assignments to tests. There are no special structures behind such group, only the name. Using the same group name with several forge assignments makes the resource belong to the scope of this group of tests. If to think about forge as about a declaration of test requirement to create a resource, the scope will define a group of tests sharing this resource. Forge <code>scope</code> is an important term in framework that allows to leverage forge setup and teardown parts executions by the framework. Scope is a part of the framework internal mechanism that allows multiply tests to have common dependency, i.e. to reuse resources created by same forge function and avoid unnecessary creations and deletions of this resource multiple times for each test. This mechanism is based on the following logic:</p> <ol> <li> <p>When framework plans execution of a forge function attached to a tests, it looks through internal records to check if this forge function has been already executed in context of a different test. </p> </li> <li> <p>Framework assumes that forge functions is not vulnerable to any side effect. In other words, if to execute the same forge function with the same argument values, it will do the same action or it will generate the same resource. So if framework finds previous execution of the forge with the same argument values, it\u2019s about to treat it as requirement for the same resource. </p> </li> <li> <p>If forge argument values are the same framework supposes that it\u2019s going to create the same resource and almost ready to make a decision to skip forge execution.</p> </li> <li> <p>The last step before making the final decision about skipping the forge execution is to make sure if this is what developer wants to be done. At this point forge scopes are taken into account. Framework checks the forge declarations at both tests - where the forge function was executed and the current tests. If they have the same scope specified then forge execution is skipped. </p> </li> </ol> <p>Knowing the above logic developer can tell framework how to treat the forge by letting forges to stay in the same scope or split them ito different scopes. Developer can use any string as a scope name, however there are three names that have special meaning for the framework:</p> <ul> <li> <p><code>session</code> - This is default scope name, which means that every forge assigned to every test has this scope if nothing different is specified. So by default all forge assignments belong to the same group for all tests, no matter in which module test is implemented. That in it\u2019s turn means that for all tests the same forge function with the same argument values will be treated by framework as creating the same resource and and will be executed only once. </p> </li> <li> <p><code>module</code> - This scope name says that forge assignment scope should be limited to all tests of the same test module (python source file). When framework registers forge assignment to a tests it does not store scope name as is, instead it replaces <code>module</code> with the path of the test module where this assignment takes place. Note that forge assignments with <code>session</code> scope and <code>module</code> scope remain in different scopes even if they are located in the same test module, i.e. <code>module</code> scope is made only of forge assignments with scope <code>module</code> and located in the same test module. In some sense value <code>module</code> is a shortcut to tests module file path, so using it in several test modules in reality creates a separate scope for each of those test modules.</p> </li> <li> <p><code>function</code> - This scope name says framework to consider forge (or resource created by it) only in context of one single tests that this forge is assigned to. This means that framework will never treat this forge as creating common reusable resource. Similar to <code>module</code> scope, <code>function</code> scope name is not saved as is, instead framework replaces <code>function</code> string with full path to the test function that includes the path to test function module source file, test function class if exists and test function name.</p> </li> </ul> <p>Knowing the forge scope (single test or a group of tests), framework can estimate the lifetime of the resource created by forge function. It monitors the moment when the last test using this resource completes and immediately invokes forge teardown part to remove the resource. </p> <p>To early catch typos in predefined scope names framework defines enum object with corresponding enum values in <code>splunk_add_on_ucc_modinput_test.functional.constants</code> module: <pre><code>class ForgeScope(Enum):\n    FUNCTION = \"function\"\n    MODULE = \"module\"\n    SESSION = \"session\"\n</code></pre> To summarize:</p> <ul> <li> <p>Forge assignment scope is about altering default framework behavior when a forge function assigned to multiply test functions.</p> </li> <li> <p>In most cases there is no need to alter default behavior, as framework can clearly understand when a forge assigned to several tests is going to create the same resource or different one judging only by forge argument values. </p> </li> <li> <p>Explicit scope assignment makes sense only when developer has something specific in mind that does not much default framework behavior and wants to tell framework to create resource for specific test or group of tests where otherwise it would reuse existing resource. </p> </li> <li> <p>Same results can be achieved by manipulation of forge argument values, however some scenarios are more difficult to implement. Easiest scenario is to tell framework that specific forge creates separate resource for each test. It can be done by using builtin forge argument <code>test_id</code> - it will make forge argument values different from test to tests and at the same time provides unique identifier to form a unique name for created resource.</p> </li> <li> <p>When adjusting framework logic by explicitly defining forge assignment scope it is recommended to review names and lifetimes of the resources created by the forge to avoid failures caused by attempting to create resources with the same name as existing ones.</p> </li> </ul>"},{"location":"framework_deepdive/#forge-assignment","title":"Forge assignment","text":"<p>For forge assignment to tests framework implements two test function decorators that create binding between forges and a test and at the same time define when forge should be invoked - before all tests execution starts or before exact test execution starts. There are also two data collection helper classes tat allow to arrange forge dependencies and apply additional builtin and custom settings like scope and probe assignment, as well as explicit/in-place forge arguments values definition. Note that the same forge function can be assigned to different test functions, however framework does not support assigning same forge function to the same test function multiply times. Order of forges in which they are listed within used decorator defines forges dependencies, which means that each preceding forge (i.e. resource it creates) becomes a requirement for the forge following it. Note that each test should declare all forges (resources) it depends on, so if too tests depend on same resources they both should have the same list of forges declared in the same order.</p>"},{"location":"framework_deepdive/#helper-data-collection-classes","title":"Helper data collection classes","text":"<p>These helper classes together allow developer to specify all forge data necessary to create internal forge object, as well as to define which forges can be executed in parallel and which sequentially.</p>"},{"location":"framework_deepdive/#forge-helper-data-collection-class","title":"<code>forge</code> helper data collection class","text":"<p>Let\u2019s start with this helper data class <code>forge</code> as it allows to specify all forge data necessary to create internal forge object. It was already used in some example of previous sections. It has only one mandatory positional argument that receives forge function itself. There are two other arguments that have special meaning for the framework - <code>probe</code> and <code>scope</code>. They are optional and if used must be specified as named arguments. The first named argument, <code>probe</code>, allows to link a probe function to the forge function assignment and by default takes value <code>None</code>, which means no probe function is assigned.  The second named argument, <code>scope</code>, defines forge assignment scope. By default scope value for all forge assignments is \u201csession\u201d if not redefined by <code>forges</code> helper data class. <code>forge</code> class constructor also allows to define argument values for assigned forge. Note that explicitly defines=d value for forge argument will have precedence over artifactory value is such exists in test artifactory. Here is an example of a forge function and this forge assignment using <code>forge</code> helper data class. <pre><code>from splunk_add_on_ucc_modinput_test.functional.constants import ForgeScope\ndef create_splunk_index(splunk_client, index_name):\n    splunk_client.create_index(index_name)\n\n@bootstrap(\n    forge(create_splunk_index, index_name=\"some_index\", scope=ForgeScope.FUNCTION, probe=wait_for_index_creation)\n)\ntest_something(splunk_client)\n    # test implementation\n</code></pre> In above example forge <code>create_splunk_index</code> is assigned to test function <code>test_something</code>. Beside forge function, <code>forge</code> helper data class constructor defines forge assignment (resource) scope as <code>function</code> and assigns <code>wait_for_index_creation</code> function as probe. Additionally it defines <code>index_name</code> argument value for the forge function, so when <code>create_splunk_index</code> function will be invoked it will receive <code>splunk_client</code> object from framework builtin argument and <code>index_name</code> value from this explicit value assignment.  </p> <p>Within <code>bootstrap</code> or <code>attach</code> assignment decorators can declare multiply forge assignments one after another. This ordered list defines the sequence of forge function execution and can be treated as forge dependencies on each other. In other words, forge A preceding forge B in this least can be taken as forge A is a pre-requirement for forge B and must be executed before forge B. For example, Splunk index should be created before add-on modular input using this index, so forge creating index should be listed before the forge creating the modular input: <pre><code>def create_splunk_index(splunk_client, index_name):\n    splunk_client.create_index(index_name)\n\ndef create_splunk_input(splunk_client, input_name):\n    splunk_client.create_input(input_name)\n\n@bootstrap(\n    forge(create_splunk_index)\n    forge(create_splunk_input)\n)\ntest_something(splunk_client)\n    # test implementation\n</code></pre></p>"},{"location":"framework_deepdive/#forges-helper-data-collection-class","title":"<code>forges</code> helper data collection class","text":"<p>This is a helper data class that allows to define at certain position of forge assignment list a sub-list of independent forges that can be executed by framework in parallel. In other words, it allows to put a set of forges in place of a single forge assignment. </p> <p>This data collection class constructor receives unlimited number of <code>forge</code> data collection object and one optional named argument <code>scope</code> that allows to define scope for the whole group of listed forges.</p> <p>This helper data collection class can be very useful if the resources to be created do not dependent on each other and though there is no reason to create them one after another. Much faster to create them in parallel.  For example, two types of modular inputs can be created independently but both depend on creation of an index they are going to use: <pre><code>def create_splunk_index(splunk_client, index_name):\n    splunk_client.create_index(index_name)\n\ndef create_input_of_type_A(splunk_client, input_name):\n    splunk_client.create_input_a(input_name)\n\n def create_input_of_type_B(splunk_client, input_name):\n    splunk_client.create_input_b(input_name)\n\n@bootstrap(\n    forge(create_splunk_index)\n    forges(\n        forge(create_input_of_type_A),\n        forge(create_input_of_type_B),\n        scope=ForgeScope.MODULE\n    )\n)\ntest_something(splunk_client)\n    # test implementation\n</code></pre> Note that in the example <code>scope</code> value is defined in <code>forges</code> and it changes default scope for every forge in its internal list. </p>"},{"location":"framework_deepdive/#forge-assignment-decorators","title":"Forge assignment decorators","text":"<p><code>bootstrap</code> and <code>attach</code> decorators take a mixed list of <code>forge</code> and <code>forges</code> instancies and do the actual assignment of each forge function to specific test function taking into account dependency information and the settings collected by the data classes. Depending on which decorator was used, assigned forges will have different execution algorithm and schedule.</p>"},{"location":"framework_deepdive/#bootstrap-decorator","title":"<code>bootstrap</code> decorator","text":"<p>In accordance to decorator name it allows to assign forges that should be executed before tests started. There are several rules that framework follows:</p> <ul> <li> <p>Framework executes sequences of bootstrap forges for all tests at the same time. It means that framework will pick first bootstrap forge from each test and execute them all in parallel, then it will go to the next bootstrap forge of each test and again executes them in parallel until there is no bootstrap forges to execute</p> </li> <li> <p>Tests themselves executed sequentially by pytest. Before letting test go framework makes sure that bootstrap forges for this tests are successfully executed.</p> </li> <li> <p>To minimize test waiting for bootstrap forges, framework reorders test functions depending on how many bootstrap forges it has assigned - tests with less bootstrap forges go to the beginning of the test execution list. This way tests with less dependencies will be executed before not waiting for all bootstrap forges to be executed. Note that this internal optimization will not work for tests that besides bootstrap also have forges assigned with <code>attach</code> decorator.</p> </li> </ul> <p>Let\u2019s take a look at the example below that has two tests with five different forges assigned - tree forges assigned to one test and two others to the other test: <pre><code>@bootstrap(\n    forge(forge_function1)\n    forge(forge_function2)\n    forge(forge_function3)\n)\ntest_something()\n    # test implementation\n\n@bootstrap(\n    forge(forge_function4)\n    forge(forge_function5)\n)\ntest_something_else()\n    # test implementation    \n</code></pre> Here are the steps that framework will take to process these tests:</p> <ol> <li> <p>Based on forge assignment information framework will build forge execution matrix like below:     <pre><code>step 1: forge_function1, forge_function4\nstep 2: forge_function2, forge_function5\nstep 3: forge_function3\n</code></pre></p> </li> <li> <p>Framework will reorder test functions sequence:     <pre><code>1. test_something_else\n2. test_something\n</code></pre></p> </li> <li> <p>Framework deploys execution of forges. Each step will be executed sequentially, however forge functions in each step will be executed simultaneously using multithreading. By default framework is configured to use 10 threads, which can be adjusted via pytest command option <code>--number-of-threads</code>. </p> </li> <li> <p>Framework finishes setup phase and let pytest to execute tests. According to updated test sequence, test <code>test_something_else</code> will be first for execution.</p> </li> <li> <p>Here framework checks <code>test_something_else</code> test requirements and keep it waiting until forge <code>forge_function5</code> is executed which happens somewhere in the middle of step 2. As soon as forge <code>forge_function5</code> is executed, test  <code>test_something_else</code> is unblocked and pytest executes it as well.</p> </li> <li> <p>After <code>test_something_else</code> test function gets executed, framework will verify if forges assigned to the test will be needed to other tets. In this example forges <code>forge_function4</code> and <code>forge_function5</code> used only with <code>test_something_else</code> tests so framework will try to execute teardown part of each forge if such exist.</p> </li> <li> <p>pytests jumps to test <code>test_something</code> where framework blocks it again until forges deploying it\u2019s requirements executed. This time it happens with <code>forge_function3</code> forge execution and actually with completion of the whole bootstrap process. In these forges teardown part is missing.</p> </li> <li> <p>As soon as <code>forge_function3</code> is executed, framework lets pytest to execute <code>test_something</code> test function.</p> </li> <li> <p>After execution of <code>test_something</code> test, framework will verify if forges assigned to the test will be needed to other tets. Again, in this example forges <code>forge_function1</code>,<code>forge_function2</code>  and <code>forge_function3</code> used only with <code>test_something</code> tests so framework will try to execute teardown part of each forge if such exist. In these forges teardown part is also missing.</p> </li> </ol> <p>Let\u2019s take a look at another test scenario using bootstrap decorator and reusable forges/resources: <pre><code>def create_splunk_index(splunk_client):\n    # code to create index\n\ndef create_some_input(splunk_client):\n    # code to create input\n\ndef upload_test_events(vendor_client):\n    # code to upload events\n\n@bootstrap(\n    forge(create_splunk_index)\n)\ntest_splunk_index_created(splunk_client, index_name)\n    assert splunk_client.get_index(index_name) is not None\n\n@bootstrap(\n    forge(create_splunk_index),\n    forge(create_some_input)\n)\ntest_some_input_created(splunk_client, input_name)\n    assert splunk_client.get_some_input(input_name) is not None\n\n@bootstrap(\n    forge(create_splunk_index),\n    forge(create_some_input),\n    forge(upload_test_events)\n)\ntest_events_uploaded(vendor_client)\n    assert vendor_client.count_uploaded_events() == 123\n</code></pre> Flowchart for this example test scenario will look as the following: </p> <p>As mentioned earlier framework analyzes names, scopes and argument values of assigned forges to decide if same forge used by several tests actually creates the same resource that those tests intend to reuse. In the above tests all reused forges are treated as creating reused resource because they use same default scope, they do not use arguments except splunk and vendor clients (clients are not counted at argument value comparison). Fore reusable resources setup and teardown sections executed once for all tests using this resource - setup part is executed before the first tests using the forge and teardown after the last test using the forge. In this example setup for <code>create_index</code> forge executed before <code>test_splunk_index_created</code> and teardown part of this forge is executed after <code>test_events_uploaded</code>.</p>"},{"location":"framework_deepdive/#attach-decorator","title":"<code>attach</code> decorator","text":"<p>This decorator defines a sequence of forges to be executed right before the tests execution they assigned to. Just like <code>bootstrap</code> decorator, <code>attach</code> decorator accepts a mixed list of <code>forge</code> and <code>forges</code> instances and executes each list item sequentially, while items inside each <code>forges</code> instance are executed all together. Let\u2019s take a look at the example below with two test function and five forges assigned to them using <code>attach</code> decorator. <pre><code>@attach(\n    forge(forge_function1)\n    forges(\n        forge(forge_function2), \n        forge(forge_function3),\n    )\n)\ntest_something()\n    # test implementation\n\n@attach(\n    forge(forge_function4)\n    forge(forge_function5)\n)\ntest_something_else()\n    # test implementation    \n</code></pre> Here are the steps that framework will take to process these tests:</p> <ol> <li> <p>As there are not bootstrap forges used framework will skip bootstrap planning and execution steps. Execution order will not be changed because of the same reason. So framework will pass control over to pytest to start execution of tests functions. </p> </li> <li> <p>As the test execution order is not changed, pytest starts from <code>test_something</code> test function. First framework will verify that there are not bootstrap dependencies for this tests, so there is nothing to wait from this part. </p> </li> <li> <p>Next framework will check if there are forges assigned using <code>attach</code> decorator (in-place forges). For <code>test_something</code> test function there are three in-place forges assigned - <code>forge_function1</code> forge that is a dependency for other to forges <code>forge_function2</code> and <code>forge_function3</code>. For them framework will build forge execution matrix like below:     <pre><code>step 1: forge_function1\nstep 2: forge_function2, forge_function3\n</code></pre></p> </li> <li> <p>Before framework deploys the forge execution matrix it will wait for bootstrap process to finish, no matter whether there are bootstrap dependencies for this tests or not. In the example there are no bootstrap process and there is nothing to wait for.</p> </li> <li> <p>Framework will deploy the matrix. Like with bootstrap forge execution matrix, each step will be executed sequentially, while forge functions in each step will be executed simultaneously using multithreading. Until the last forge function of execution matrix gets executed, <code>test_something</code> test function remains blocked by framework.</p> </li> <li> <p>Framework unblocks test and let it be executed by pytest engine.</p> </li> <li> <p>After execution of <code>test_something</code> test, framework will verify if forges assigned to the test will be needed to other tets. Again, in this example forges <code>forge_function1</code>,<code>forge_function2</code>  and <code>forge_function3</code> used only with <code>test_something</code> tests so framework will try to execute teardown part of each forge if such exist. In these forges teardown part is also missing.</p> </li> <li> <p>Now framework lets pytest to switch to the next test which in our example is <code>test_something_else</code>.</p> </li> <li> <p>Again framework prepares forge execution matrix for the test     <pre><code>step 1: forge_function4\nstep 2: forge_function5\n</code></pre></p> </li> <li> <p>\u2026 verifies if the test should wait for its bootstrap dependencies and bootstrap precess as a whole, runs forges execution matrix.</p> </li> <li> <p>When all forges are executed framework let\u2019s pytest to run the test.</p> </li> <li> <p>After test execution, framework invokes teardown sections of the forges no longer required by other tests. In above example <code>test_something_else</code> is the last tests so no resources a needed anymore so framework will try to invoke teardown for remaining forges - <code>forge_function4</code> and <code>forge_function5</code>. However those forges do not have teardown sections so at the end no cleanup code will be invoked.</p> </li> </ol>"},{"location":"framework_deepdive/#when-using-bootstrap-and-attach-decorators-together-with-the-same-test","title":"When using <code>bootstrap</code> and <code>attach</code> decorators together with the same test","text":"<p>Tests can have forges assigned using both decorators at the same time. In general execution flow for tests having both types of forges will look a combination of the two previous tests:</p> <ol> <li> <p>Framework executes test bootstrap forges in together with other tests bootstrap forges</p> </li> <li> <p>Test waits for it\u2019s turn according to pytest item list</p> </li> <li> <p>Framework executes test attached forges</p> </li> <li> <p>pytest executes the test itself</p> </li> <li> <p>Framework executes teardown sections of no longer needed resources</p> </li> </ol> <p>As seen from previous sections, attached forges always executed after bootstrap process is fully complete. In practice it means that test sorting done by framework for optimization purposed will not move tests with attached forges to the beginning of the list even if such tests does not have bootstrap forges at all, - in contrast such tests will be places after the last test without attached forges and with maximum number of bootstrap forges.</p> <p>Lets take a look at one more example: <pre><code>@attach(\n    forge(forge_function1)\n    forges(\n        forge(forge_function2), \n        forge(forge_function3),\n    )\n)\ntest_something()\n    # test implementation\n\n@bootstrap(\n    forge(forge_function4)\n)\n@attach(\n    forge(forge_function5)\n    forge(forge_function6)\n)\ntest_something_else()\n    # test implementation    \n\n@bootstrap(\n    forge(forge_function7)\n)\ntest_something_more()\n    # test implementation    \n</code></pre> In the above example framework will reorder tests the following way: <pre><code>step1: test_something_more\nstep2: test_something\nstep3: test_something_else\n</code></pre> Test <code>test_something_more</code> goes to the beginning of the list because it does not have attached forges and has minimum number of bootstrap forges. Tests <code>test_something</code> and test_something_else have attached forges so they go to the end of the list. However <code>test_something</code> has less bootstrap forges then <code>test_something_else</code>, so it lends in the list before <code>test_something_else</code> test. Framework logs forge execution matrices and test execution order in log file splunk-add-on-ucc-modinput-test-functional.log so developer can review it when have doubts.</p> <p>Note that it\u2019s not allowed to have the same forge assigned to the same test more than once, no matter if only one or both decorators are used. In other words, if forge is listed in one of the decorators, it cannot be listed neither in the same decorator second time or appear in the list of the other decorator.</p>"},{"location":"framework_deepdive/#support-for-parametrized-test-arguments","title":"Support for parametrized test arguments","text":"<p>Framework supports test parametrized arguments by making them available for forges and probes via function arguments. Below is an example showing how to access parametrized arguments in forges and tests. Test declares <code>input_type</code> parametrize argument that takes tree possible values. Just like test function, to use this parametrized argument example forge and probe functions should declare argument with the same name in their function arguments. Note that parametrized argument is not mandatory for forges and probes, in contrast to test function they need to declare this argument only if they are going to use it.</p> <pre><code># forge to create an input of specific type and with specific name\ndef create_some_input(splunk_client, input_type, test_id):\n    input_name = f'{input_type}_{test_id}'\n    splunk_client.create_input_of_type(input_type, input_name)  # forge code implemented by developer to create inputs\n    return dict(\n        input_name = input_name,\n        input_type = input_type\n    )\n\n# probe to wait for input to be created\ndef is_input_created(splunk_client, input_type, input_name):\n    return splunk_client.get_input(input_type, input_name) is not None  # probe code to verify that input exists\n\n@pytest.mark.parametrize(\n    \"input_type\",\n    [\n        \"input_type_name1\", \n        \"input_type_name4\", \n        \"input_type_name3\"]\n    ],\n)\n@bootstrap(\n    forge(create_some_input, probe=is_input_created),\n)\ndef test_input_started_successfully(input_type):    \n    # some verification code\n</code></pre>"},{"location":"framework_deepdive/#probes","title":"Probes","text":"<p>The single purpose of a probe is to do a check that certain resource is created or required conditions met. Probes are used by the framework together with forges and let it verify that an action taken by a forge achieved the expected result and if it\u2019s not to wait for expected result if necessary. This means that if for some reason result of the probe is negative, framework will keep calling the probe in certain intervals until it gets successful or the time configured for waiting the expected result expires. Though framework does not jump to the next following forge execution until probe succeeds or expires. Probes can use any parameters saved in test artifactory by declaring them in probe function arguments. There are two ways do implement a probe supported by the unified functional test framework - using function or using generator function. Depending on the approach chosen, developer will have different control on the verification process and requirements to probe return values.</p>"},{"location":"framework_deepdive/#probe-as-function","title":"Probe as function","text":"<p>As follows from the name, this kind of probe is a regular function returning True or False, depending on if it was successful or failed accordingly. So it\u2019s pretty simple to implement and developer is required just to create straightforward code checking some desired condition. Framework in its turn is responsible for calling this probe with default frequency until it gets successful or default timeout is reached. The probe call interval and expiration period for all such probes are defined globally and can be controlled vial pytest command prompt arguments <code>--probe-invoke-interval=\\&lt;value in seconds\\&gt;</code> and <code>--probe-wait-timeout=\\&lt;value in seconds\\&gt;</code> correspondingly. In case of any exception raised inside probe ite will be taken as permanently failed without any following attempts to call it again and the whole corresponding test will be marked as failed as well. The same way probe timeout rises internal framework SplTaFwkWaitForProbeTimeout exception that fails the probe together with corresponding forge and the test.  Note that this kind of probe does not know about how many time it was called, if the current call is the first for the forge or consequent, what is the elapse time of waiting for a check to succeed. </p> <p>Here how this type of probe may look: <pre><code>def some_input_is_created(splunk_client: SplunkClient, input_name: str) -&gt; bool:\n    return splunk_client.get_some_input(input_name) is not None\n</code></pre></p>"},{"location":"framework_deepdive/#probe-as-generator-function","title":"Probe as generator function","text":"<p>This approach is more complicated and requires developer to create a generator which fulfils required protocol to interact with the framework:  - in case of unsuccessful check this generator should yield integer positive value in seconds that framework should use as interval before calling probe once again. Framework verifies yielded interval value and makes sure it\u2019s within 1-60. Framework will update interval with minimum or maximum value of the expected range in case yielded interval value is less than the range minimum or bigger than the range maximum correspondingly. - if the check was successful, generator should exit optionally returning True.  - if probe has internally defined timeout which is less than global probe timeout, the probe can gracefully exit returning False or through an exception. - if probe does not have internal timeout or internal timeout is greater then global probe timeout the framework will raise internal probe timeout exception when probing process time exceeded global probe timeout.</p> <p>Here how this type of probe may look: <pre><code>def some_input_is_created(splunk_client: SplunkClient, input_name: str) -&gt; Generator[int, None, Optional[bool]] :\n    timeout = 60\n    start_time = time()\n    # can have here some preliminary preparations or checks\n    while time() - start_time &lt; timeout:\n        success = splunk_client.get_some_input(input_name) is not None\n        if success:\n            return True\n        yield 10\n\n    return False # or raise and exception\n</code></pre> As seen from the example this type of probe is aware about probing progress and has more control over it:</p> <ul> <li> <p>it defines check interval and can vary it depending on progress conditions</p> </li> <li> <p>it can decide if with probe failing also to fail the test or exit gracefully giving a chance to a test to decide how to treat probe failing</p> </li> <li> <p>it can have some kind of init code for preliminary preparations and checks.</p> </li> </ul>"},{"location":"framework_deepdive/#helper-search-probe-as-generator-function","title":"Helper search probe as generator function","text":"<p>To make creation of generator function probes easier framework provides a default probe as a methods of splunk_client built in argument. The probe is based on search operation in Splunk index which is most popular way of probing when it\u2019s needed to make sure that expected events have been ingested or logs have been generated by an add-on code. A probe using this helper probe will look like the following: <pre><code>def wait_for_some_input_to_start(\n    splunk_client: SplunkClient\n) -&gt; Generator[int, None, True]:\n    probe_spl = \"some SPL looking into Splunk _internal index for a log generated by input process at start\"\n    successful = yield from splunk_client.search_probe(\n        probe_spl,      # the SPL to search\n        timeout=30,     # maximum time in seconds given to get successful result. It's an optional argument with default value 300\n        interval=10,    # interval of prove invocation. It's an optional argument with default value 5\n        verify_fn = my_verify_function,  # optional function to search result analysis return ing True/False. \n                                        # by default the probe is successful search returns at least one record\n        probe_name=\"wait_for_some_input_to_start\" # optional name of your probe used only for test logging\n    )\n    return successful\n</code></pre> As seen from the example comments, only probe_spl argument is mandatory to call this helper probe.</p> <p>By defining your own verification function (verify_fn argument) it\u2019s possible to alter expected condition for positive result. By default it expects from SPL any non empty result. Custom verify function like below will make it expect some specific number of events , let it be 10: <pre><code>def my_verify_function(state: SearchState) -&gt; bool:\n    return state.result_count == 10\n</code></pre></p>"},{"location":"framework_deepdive/#probe-arguments-and-return-value","title":"Probe arguments and return value","text":"<p>To summarize, a probe can rely on any built in framework argument or any artefact (a property stored in test artifactory) just by declaring probe function arguments with the same names as expected artifacts. It\u2019s not possible to pass to a probe any argument value explicitly, but it\u2019s possible to do it via argument of the the forge this probe is assigned to and this forge should saves the argument in test artifactory. Probe can return a boolean value. If it does, framework will handle it and add to test artifactory with the name of probe function as the key and the returned boolean value as the value. This artefact can then be used by the test and by other probes and forges executed at later processing stages.</p>"},{"location":"framework_deepdive/#test","title":"Test","text":"<p>Test is an any function recognized by pytest as test function. Test can be a regular function or test class method. To become a part of unified functional test framework workflow a test must have at least one the forge assignment decorators applied.</p>"},{"location":"framework_deepdive/#test-execution-order","title":"Test execution order","text":"<p>As explained in previous sections framework reorders tests based on assigned forges to improve overall test execution time. During this process framework reorders all the tests no matter if it\u2019s a part of the framework or not. . All tests that are not part of the framework (i.e. do not have forges assigned) will go to the beginning of the execution list and will be invoked right after framework bootstrap forge execution stats and not waiting for it completion.</p>"},{"location":"framework_deepdive/#test-arguments","title":"Test arguments","text":"<p>Being a part of framework tests gets access to test artifactory and framework builtin arguments like splunk_client, vendor_client, test_is, session_id and so on. Below is an example demonstration how those arguments can be used in test: <pre><code># forge to create an input of specific type and with specific name\ndef create_some_input(splunk_client, input_type, test_id):\n    input_name = f'{input_type}_{test_id}'\n    splunk_client.create_input_of_type(input_type, input_name)  # forge code impemented by developer to create inputs\n    return dict(\n        input_type=input_type,\n        input_name=input_name\n    )\n\n# probe to wait for input to be created\ndef is_input_created(splunk_client, input_type, input_name):\n    return splunk_client.get_input(input_type, input_name) is not None  # probe code to verify that input exists\n\n@bootstrap(\n    forge(create_some_input, input_type=\"some_type\", probe=is_input_created),\n)\ndef test_input_started_successfully(splunk_client, test_id, input_type, input_name, is_input_created)\n    assert is_input_created is True # make sure probe did not timeout\n    assert test_id in input_name and input_type in input_name # make sure input name generated correctly\n    assert splunk_client.look_for_input_initialization_logs(input_name) # some method implemented by developer \n                                                                        # to search for input initialization logs \n                                                                        # to make sure it's started successfully\n</code></pre> As seen from above <code>test_input_started_successfully</code> declares five arguments provided by the framework:</p> <ul> <li> <p><code>splunk_client</code> and <code>test_id</code> are built in framework arguments providing for tests an instance of Splunk client class and internal framework test id.</p> </li> <li> <p><code>input_type</code> and  <code>input_name</code> are two variables from test artifactory saved there by <code>create_some_input</code> forge.</p> </li> <li> <p><code>is_input_created</code> is a variable created by framework to store last result of probe function which contains result of the last probe execution. Checking it allows to verify whether probe was executed with positive result or it was not successful and framework stopped executing it because of the timeout.</p> </li> </ul>"},{"location":"framework_deepdive/#tasks","title":"Tasks","text":"<p>Task is not what developer is going to deal with directly. Task is an internal framework entity that is a combination of a forge function with specific argument values and optionally attached probe that should be executed for a specific test.</p>"},{"location":"framework_deepdive/#forge-execution-flow-and-resource-lifetime-diagram","title":"Forge execution flow and resource lifetime diagram","text":"<p>Below is forge execution flow diagram for an example test set. Diagram demonstrates how framework default logic works when it schedules and processes tests having different amounts and types of forges assigned. Example is simplified the way that forges do not take arguments and do not have scopes altered, so framework will treat reuse of a forges as reuse of the resources created by them. In other words, when multiple test have the same forge function assigned it\u2019s taken by framework that all these tests are going to reused the same resource. By design, for a reused resource framework executes corresponding forge setup and teardown sections only once, despite the fact that forge is assigned to multiply tests - setup part is executed before the first tests using the forge and teardown after the last test using the forge. In the diagram skipped forge setup executions for reused resources marked with red cross.</p> <p>I the diagram the very left column contains tests (without bodies) with forge assignment headers showing the binding of the tests and the forges. In the middle of the diagram there is a time arrow splitting bootstrap forges execution flow on the left and other test and forge execution states on the right. On the left side each column shows forge execution flow for each separate test. On the right each column is a test execution state reflecting test and forges in this state at certain moment of time.  Note:</p> <ul> <li> <p>Assuming that primary forge purpose is creation of resources at vendor and Splunk side, term resource lifetime means time interval between forge setup and teardown invocations</p> </li> <li> <p>In a real live forges will do have arguments. When comparing forge argument values framework ignores arguments representing Splunk and vendor clients.</p> </li> </ul>"},{"location":"framework_deepdive/#best-practices","title":"Best practices","text":"<ul> <li> <p>Make sure that what forge do and what it returns depends only on forge input arguments values. Avoid using random stings or mutating side values when creating names for resources generated by a forge. This is what framework assumes forges do.</p> </li> <li> <p>Try to use bootstrap forges where possible. They executed all together for all tests that makes tests execution faster. Use attached forges only when bootstrap forge unnecessary complicates test logic or when bootstrap forge cannot be used at all (for example, when configuring single instance resources, like add-on proxy)</p> </li> <li> <p>Use test function only for asserts/verifications and if needed for collecting data required for verification. Avoid using it to create resources or apply configurations - those actions should be done in forges. This approach gives more space to parallelize execution process</p> </li> <li> <p>Let forge do one thing while keeping balance between size of forges and size of test assignment header. So it can be one small thing or one big thing.</p> </li> <li> <p>Name forges the way that makes it clear what it does when you see it assigned to a test.</p> </li> </ul>"},{"location":"framework_deepdive/#troubleshooting","title":"Troubleshooting","text":""},{"location":"framework_deepdive/#troubleshooting-resources","title":"Troubleshooting resources","text":""},{"location":"framework_deepdive/#log-file-splunk-add-on-ucc-modinput-test-functionallog","title":"Log file splunk-add-on-ucc-modinput-test-functional.log","text":"<p>This is low level framework log. Everything logged goes there from internal framework implementation. This log file collects detail information about tests, probes and forges execution, forge execution schedules, updated test order and so on. Below are several log examples:</p>"},{"location":"framework_deepdive/#informational-log-examples","title":"Informational log examples","text":"<ul> <li> <p>Example of logged test execution order     <pre><code>Test execution order:\n0. .../tests/ucc_modinput_functional/test_canary.py::test_canary_boostrap\n    Level 0\n        .../tests/ucc_modinput_functional/splunk/forges/canary.py::splunk_canary_forge\n        .../tests/ucc_modinput_functional/vendor/forges/canary.py::vendor_canary_forge\n1. .../tests/ucc_modinput_functional/test_canary.py::test_canary_attach\n2. .../tests/ucc_modinput_functional/test_settings.py::test_valid_loglevel\n3. .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite0-Error in validating proxy configuration]\n4. .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite1-All of the following errors need to be fixed: [\"Not matching the pattern: ^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])\\\\\\\\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9-]*[A-Za-z0-9])$\"]]\n5. .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite2-Invalid format for numeric value]\n6. .../tests/ucc_modinput_functional/test_canary.py::test_canary_mixed\n    Level 0\n        .../tests/ucc_modinput_functional/splunk/forges/canary.py::splunk_canary_forge\n</code></pre></p> </li> <li> <p>Example of logged bootstrap forges execution matrix     <pre><code>Bootstrap Dependency execution matrix:\nStep 1:\n    test 1: .../tests/ucc_modinput_functional/test_canary.py::test_canary_boostrap\n        Dependency .../tests/ucc_modinput_functional/splunk/forges/canary.py::splunk_canary_forge, scope session\n        Dependency .../tests/ucc_modinput_functional/vendor/forges/canary.py::vendor_canary_forge, scope session\n    test 2: .../tests/ucc_modinput_functional/test_canary.py::test_canary_attach\n        No dependencies at this step\n    test 3: .../tests/ucc_modinput_functional/test_canary.py::test_canary_mixed\n        Dependency .../tests/ucc_modinput_functional/splunk/forges/canary.py::splunk_canary_forge, scope session\n    test 4: .../tests/ucc_modinput_functional/test_settings.py::test_valid_loglevel\n        No dependencies at this step\n    test 5: .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite0-Error in validating proxy configuration]\n        No dependencies at this step\n    test 6: .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite1-All of the following errors need to be fixed: [\"Not matching the pattern: ^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])\\\\\\\\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9-]*[A-Za-z0-9])$\"]]\n        No dependencies at this step\n    test 7: .../tests/ucc_modinput_functional/test_settings.py::test_proxy_validators__invalid_params[overwrite2-Invalid format for numeric value]\n        No dependencies at this step\n</code></pre></p> </li> <li> <p>logs about waiting for something</p> <ul> <li> <p>waiting fro bootstrap <pre><code>2025-02-25 14:19:06.263 DEBUG splunk-add-on-ucc-modinput-test-functional pid=83518 tid=8601208896 file=manager.py func=wait_for_test_bootstrap line=402 &lt;Test /Users/okashaev/TA/github/splunk-add-on-for-google-cloud-platform/tests/ucc_modinput_functional/test_google_cloud_rh_inputs_monitoring.py::test_valid_monitoring_input&gt; is waiting for bootstrap dependencies\n</code></pre></p> </li> <li> <p>waiting for attached forges <pre><code>2025-02-25 14:19:36.294 DEBUG splunk-add-on-ucc-modinput-test-functional pid=83518 tid=8601208896 file=executor.py func=wait line=230 Still waiting for executor to process all attached tasks\n</code></pre></p> </li> <li> <p>waiting for a probe <pre><code>2025-03-13 09:50:38.847 DEBUG splunk-add-on-ucc-modinput-test-functional pid=30209 tid=12922871808 file=task.py func=wait_for_probe line=272 WAIT FOR PROBE\n    test ('.../tests/ucc_modinput_functional/test_settings.py', 'test_valid_loglevel2')\n    forge ('.../tests/ucc_modinput_functional/splunk/forges/settings.py', 'set_loglevel', 'session')\n    probe &lt;function wait_for_loglevel at 0x7fde3009ee60&gt;\n    probe_gen &lt;function wait_for_loglevel at 0x7fde3009ee60&gt;\n    probe_args {'splunk_client': &lt;tests.ucc_modinput_functional.splunk.client.splunk_client.SplunkClient object at 0x7fde70f70790&gt;, 'expected_loglevel': 'CRITICAL'}\n</code></pre></p> </li> <li>Logs the moment when forge was marked as executed. If this is the last test dependency/requirement framework triggers execution of the test itself.  <pre><code>2025-03-13 09:50:39.211 DEBUG splunk-add-on-ucc-modinput-test-functional pid=30209 tid=12922871808 file=task.py func=mark_as_executed line=311 MARK TASK EXECUTED: .../tests/ucc_modinput_functional/splunk/forges/settings.py::set_loglevel,\n    self id: 140592265375440,\n    scope: session,\n    exec_id: 1741855836527287152,\n    test: ('.../tests/ucc_modinput_functional/test_settings.py', 'test_valid_loglevel2'),\n    is_executed: True,\n    is_failed: False,\n    errors: []\n</code></pre></li> <li>Successful forge execution: <pre><code>2025-03-26 15:16:49.269 INFO splunk-add-on-ucc-modinput-test-functional pid=5628 tid=12955860992 file=task.py func=execute line=448 Forge has been executed successfully, time taken 0.851635217666626 seconds:\ntest: test_proxy_validators__invalid_params[overwrite1-Bad Request -- Invalid format for integer value],\n    location: .../tests/ucc_modinput_functional/test_settings.py,\n    forge: try_to_configure_proxy,\n        location: .../tests/ucc_modinput_functional/splunk/forges.py,\n        scope: session,\n        exec id: 1742998608417570622,\n        kwargs: {'overwrite': {'proxy_port': 'not-a-number'}, 'splunk_client': &lt;tests.ucc_modinput_functional.splunk.client.client.SplunkClient object at 0x7fd2e88c98d0&gt;},\n    probe: None,\n        location: None,\n        kwargs: {},\n</code></pre></li> <li>Probe related information</li> <li>Information about probe waiting started:     <pre><code>2025-03-26 15:16:49.270 DEBUG splunk-add-on-ucc-modinput-test-functional pid=5628 tid=12955860992 file=task.py func=wait_for_probe line=289 WAIT FOR PROBE started\n    test ('.../tests/ucc_modinput_functional/test_settings.py', 'test_proxy_validators__invalid_params[overwrite1-Bad Request -- Invalid format for integer value]')\n    forge ('.../tests/ucc_modinput_functional/splunk/forges.py', 'try_to_configure_proxy', 'session')\n    probe None\n</code></pre></li> <li>Information about keeping waiting for a probe     <pre><code>2025-03-13 09:50:38.847 DEBUG splunk-add-on-ucc-modinput-test-functional pid=30209 tid=12922871808 file=task.py func=wait_for_probe line=272 WAIT FOR PROBE\n    test ('.../tests/ucc_modinput_functional/test_settings.py', 'test_valid_loglevel2')\n    forge ('.../tests/ucc_modinput_functional/splunk/forges/settings.py', 'set_loglevel', 'session')\n    probe &lt;function wait_for_loglevel at 0x7fde3009ee60&gt;\n    probe_gen &lt;function wait_for_loglevel at 0x7fde3009ee60&gt;\n    probe_args {'splunk_client': &lt;tests.ucc_modinput_functional.splunk.client.splunk_client.SplunkClient object at 0x7fde70f70790&gt;, 'expected_loglevel': 'CRITICAL'}\n</code></pre></li> </ul> </li> <li> <p>Sequence of logs related to teardown </p> <ul> <li> <p>Teardown is detected and registered     <pre><code>2025-03-14 13:44:59.917 DEBUG splunk-add-on-ucc-modinput-test-functional pid=56915 tid=13014884352 file=forge.py func=add line=110 REGISTER TEARDOWN 1741956297949821364: \nTeardown summary:\n    data.id: 1741956297949821364,\n    data.count=1,\n    data.is_teardown_executed=False\n    data.teardown=&lt;generator object set_loglevel at 0x7fba30567d50&gt;\n    data.kwargs={'loglevel': 'CRITICAL'}\n    data.result={'expected_loglevel': 'CRITICAL', 'wait_for_loglevel': True}\n    teardown_is_blocked=False\n</code></pre></p> </li> <li> <p>Teardown information:     <pre><code>2025-03-26 15:16:49.295 DEBUG splunk-add-on-ucc-modinput-test-functional pid=5628 tid=8606839296 file=task.py func=teardown line=487 Teardown task\n    _exec_id: 1742998608417570622\n    forge: .../tests/ucc_modinput_functional/splunk/forges.py::try_to_configure_proxy,\n    scope: session,\n    task: ('.../tests/ucc_modinput_functional/test_settings.py', 'test_proxy_validators__invalid_params[overwrite1-Bad Request -- Invalid format for integer value]')\n    teardown &lt;generator object try_to_configure_proxy at 0x7fd31aeea1d0&gt;\n</code></pre></p> </li> <li> <p>Teardown state before execution     <pre><code>2025-03-26 15:16:49.295 DEBUG splunk-add-on-ucc-modinput-test-functional pid=5628 tid=8606839296 file=forge.py func=dereference_teardown line=165 BEFORE EXECUTE TEARDOWN 1742998608417570622:\nTeardown summary:\n    data.id: 1742998608417570622,\n    data.count=1,\n    data.is_teardown_executed=False\n    data.teardown=&lt;generator object try_to_configure_proxy at 0x7fd31aeea1d0&gt;\n    data.kwargs={'overwrite': {'proxy_port': 'not-a-number'}}\n    data.result={'expected_proxy': {'proxy_enabled': '0', 'proxy_port': 'not-a-number', 'proxy_rdns': '1', 'proxy_type': 'http', 'proxy_url': 'localhost', 'proxy_username': 'some_user_name', 'proxy_password': 'some_password'}, 'error': 'Unexpected error \"&lt;class \\'splunktaucclib.rest_handler.error.RestError\\'&gt;\" from python handler: \"REST Error [400]: Bad Request -- Invalid format for numeric value\". See splunkd.log/python.log for more details.', 'status_code': 500}\n    teardown_is_blocked=False\n</code></pre></p> </li> <li> <p>Teardown check to execute or postpone teardown execution     <pre><code>2025-03-26 15:16:49.295 DEBUG splunk-add-on-ucc-modinput-test-functional pid=5628 tid=8606839296 file=forge.py func=exec_teardown_if_ready line=92 CAN EXECUTE TEARDOWN True:\nTeardown summary:\n    data.id: 1742998608417570622,\n    data.count=0,\n    data.is_teardown_executed=False\n    data.teardown=&lt;generator object try_to_configure_proxy at 0x7fd31aeea1d0&gt;\n    data.kwargs={'overwrite': {'proxy_port': 'not-a-number'}}\n    data.result={'expected_proxy': {'proxy_enabled': '0', 'proxy_port': 'not-a-number', 'proxy_rdns': '1', 'proxy_type': 'http', 'proxy_url': 'localhost', 'proxy_username': 'some_user_name', 'proxy_password': 'some_password'}, 'error': 'Unexpected error \"&lt;class \\'splunktaucclib.rest_handler.error.RestError\\'&gt;\" from python handler: \"REST Error [400]: Bad Request -- Invalid format for numeric value\". See splunkd.log/python.log for more details.', 'status_code': 500}\n    teardown_is_blocked=False\n</code></pre></p> </li> <li> <p>Teardown post execution state (logged only if teardown was executed)     <pre><code>2025-03-26 15:16:49.295 INFO splunk-add-on-ucc-modinput-test-functional pid=5628 tid=8606839296 file=forge.py func=dereference_teardown line=174 Teardown has been executed successfully, time taken: 0.00016307830810546875 seconds:\nTeardown summary:\n    data.id: 1742998608417570622,\n    data.count=0,\n    data.is_teardown_executed=True\n    data.teardown=&lt;generator object try_to_configure_proxy at 0x7fd31aeea1d0&gt;\n    data.kwargs={'overwrite': {'proxy_port': 'not-a-number'}}\n    data.result={'expected_proxy': {'proxy_enabled': '0', 'proxy_port': 'not-a-number', 'proxy_rdns': '1', 'proxy_type': 'http', 'proxy_url': 'localhost', 'proxy_username': 'some_user_name', 'proxy_password': 'some_password'}, 'error': 'Unexpected error \"&lt;class \\'splunktaucclib.rest_handler.error.RestError\\'&gt;\" from python handler: \"REST Error [400]: Bad Request -- Invalid format for numeric value\". See splunkd.log/python.log for more details.', 'status_code': 500}\n    teardown_is_blocked=False\n</code></pre></p> </li> <li> <p>Teardown post execution state (always logged no matter if teardown was executed or postponed)     <pre><code>2025-03-26 15:16:49.295 DEBUG splunk-add-on-ucc-modinput-test-functional pid=5628 tid=8606839296 file=forge.py func=dereference_teardown line=176 AFTER EXECUTE TEARDOWN 1742998608417570622:\nTeardown summary:\n    data.id: 1742998608417570622,\n    data.count=0,\n    data.is_teardown_executed=True\n    data.teardown=&lt;generator object try_to_configure_proxy at 0x7fd31aeea1d0&gt;\n    data.kwargs={'overwrite': {'proxy_port': 'not-a-number'}}\n    data.result={'expected_proxy': {'proxy_enabled': '0', 'proxy_port': 'not-a-number', 'proxy_rdns': '1', 'proxy_type': 'http', 'proxy_url': 'localhost', 'proxy_username': 'some_user_name', 'proxy_password': 'some_password'}, 'error': 'Unexpected error \"&lt;class \\'splunktaucclib.rest_handler.error.RestError\\'&gt;\" from python handler: \"REST Error [400]: Bad Request -- Invalid format for numeric value\". See splunkd.log/python.log for more details.', 'status_code': 500}\n    teardown_is_blocked=False\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"framework_deepdive/#error-log-examples","title":"Error log examples","text":"<p>Note that for any error message or handled exception framework collects and logs the following information: error message itself, information about test being processed together with information about the forge being executed, attached probe, collected forge and probe arguments, stack trace. </p> <p>When looking in the logs for an issue it\u2019s recommendes first to searching for <code>tracepack</code> instead of <code>error</code> - word <code>error</code> is used in data structures and can result in lots of useless matches. Many errors can be results of other errors happened earlier, so it\u2019s recommended to start analysis from the very first error.</p> <ul> <li>Example of log for forge execution fail due to missing mandatory environment variable     <pre><code>2025-03-26 14:04:56.746 ERROR splunk-add-on-ucc-modinput-test-functional pid=16704 tid=12978290688 file=task.py func=mark_as_failed line=331 Failed to prepare forge call args: Mandatory environment variable MSCS_AZURE_ACCOUNT_CONF_ENCODED is             missing and does not have a default value specified.\ntest: test_canary_boostrap,\n    location: .../tests/ucc_modinput_functional/test_canary.py,\n    forge: vendor_canary_forge,\n        location: .../tests/ucc_modinput_functional/vendor/forges/canary.py,\n        scope: session,\n        exec id: None,\n        kwargs: {},\n    probe: vendor_canary_probe,\n        location: .../tests/ucc_modinput_functional/vendor/probes/canary.py,\n        kwargs: {},\nTraceback (most recent call last):\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/functional/executor.py\", line 100, in _process_test_tasks\n    self._global_builtin_args_factory(task.test_key)\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/functional/executor.py\", line 207, in global_builtin_args_factory\n    return self._manager.get_global_builtin_args(test_key)\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/functional/manager.py\", line 190, in get_global_builtin_args\n    ] = self.create_global_builtin_args()\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/functional/manager.py\", line 167, in create_global_builtin_args\n    v_conf_instance = v_config(self._pytest_config)\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/functional/vendor/configuration.py\", line 10, in __init__\n    self.customize_configuration()\nFile \".../tests/ucc_modinput_functional/vendor/client/configuration.py\", line 10, in customize_configuration\n    \"MSCS_AZURE_ACCOUNT_CONF_ENCODED\", string_function=utils.Base64.decode\nFile \".../.venv/lib/python3.7/site-packages/splunk_add_on_ucc_modinput_test/common/utils.py\", line 77, in get_from_environment_variable\n    raise SplunkClientConfigurationException(error)\nsplunk_add_on_ucc_modinput_test.common.utils.SplunkClientConfigurationException: Mandatory environment variable MSCS_AZURE_ACCOUNT_CONF_ENCODED is             missing and does not have a default value specified.\n</code></pre></li> </ul>"},{"location":"framework_deepdive/#log-file-ucc_modinput_testlog","title":"Log file ucc_modinput_test.log","text":"<p>This is high level log that comes from the logger provided by framework in <code>splunk_add_on_ucc_modinput_test/__init__.py</code>. It\u2019s recommended to use this logger when log information from test, probes, forges and other code related to unified functional test implementation.</p>"},{"location":"framework_deepdive/#framework-sequential-execution-mode","title":"Framework sequential execution mode","text":"<p>By default to execute forges and probes framework uses multithreading. This allows to speed up overall test execution but makes it more difficult to debug in case a forge or a probe have bugs. Sequential execution mode allows to start tests without using multithreading. All forges and probes executed sequentially which is much more easier for debugging. Sequential mode can be turned on by using <code>--sequential-execution</code> pytest custom command argument flag. When this flag is used framework ignores value of <code>--number-of-threads</code> and does not creates any threads.</p>"},{"location":"framework_deepdive/#sample-issues-and-troubleshooting","title":"Sample issues and troubleshooting","text":""},{"location":"framework_deepdive/#assertion-attempt-to-assign-the-same-forge-multiply-times-or-duplicated-test-name","title":"Assertion \u201cAttempt to assign the same forge multiply times or duplicated test name\u201d","text":"<p>This error message is logged to <code>splunk-add-on-ucc-modinput-test-functional.log</code> by test. As follows from the error message this error can be cased by two reasons:</p> <ul> <li> <p>The same forge is assigned to the same tests more then once</p> </li> <li> <p>There are more then one test with the same name that assigns te same forge. It sometime happens when copy-pasting tests together with assignment header. </p> </li> </ul>"},{"location":"hello_world/","title":"Hello World example","text":"<p>This, step by step, instruction uses Splunk_TA_Example to show how you can create end to end, functional, modinput tests for your add-on.</p> <p>If you want to make a lab exercise, clone the repository to your workstation and create dedicated directory for the tests (eg. splunk-example-ta-test), so it can look like: <pre><code>.\n\u251c\u2500\u2500 splunk-example-ta\n\u2514\u2500\u2500 splunk-example-ta-test\n</code></pre></p>"},{"location":"hello_world/#satisfy-prerequisites","title":"Satisfy prerequisites","text":"<p>Open <code>splunk-example-ta/</code> in terminal.</p> Click to check where we are with the prerequisites <ul> <li> <p> Prepared basic setup for the add-on</p> <ul> <li> <p> Vendor product configured for the add-on</p> </li> <li> <p> Splunk instance with add-on installed</p> </li> <li> <p> The setup is manually tested</p> </li> </ul> </li> <li> <p> openapi.json saved to developer workstation</p> </li> <li> <p> docker installed and started</p> </li> </ul> <p>Example TA for Splunk comes with script that automates environment setup.</p> <p>The script requires docker, so make sure that docker installed and started.</p> Click to check where we are with the prerequisites <ul> <li> <p> Prepared basic setup for the add-on</p> <ul> <li> <p> Vendor product configured for the add-on</p> </li> <li> <p> Splunk instance with add-on installed</p> </li> <li> <p> The setup is manually tested</p> </li> </ul> </li> <li> <p> openapi.json saved to developer workstation</p> </li> <li> <p> docker installed and started</p> </li> </ul> <p>Run following script  <pre><code>./scripts/run_locally.sh\n</code></pre> to get:</p> <ul> <li> <p>server-example-ta that exposes <code>events</code> endpoint on port 5000</p> </li> <li> <p>splunk-example-ta that is Splunk instance exposing standard ports (we\u2019ll be interested in 8000 - web and 8089 - management port) with example TA installed.</p> </li> </ul> Click to check where we are with the prerequisites <ul> <li> <p> Prepared basic setup for the add-on</p> <ul> <li> <p> Vendor product configured for the add-on</p> </li> <li> <p> Splunk instance with add-on installed</p> </li> <li> <p> The setup is manually tested</p> </li> </ul> </li> <li> <p> openapi.json saved to developer workstation</p> </li> <li> <p> docker installed and started</p> </li> </ul> <p>There is another script that creates Example TA configuration and inputs:</p> <pre><code>./scripts/local_testing_setup.sh\n</code></pre> <p>You can verify both scripts results by:</p> <ul> <li> <p>opening the splunk instance: http://localhost:8000</p> </li> <li> <p>signing in (admin / Chang3d!)</p> </li> <li> <p>checking configuration and inputs</p> </li> <li> <p>searching events</p> </li> </ul> Click to check where we are with the prerequisites <ul> <li> <p> Prepared basic setup for the add-on</p> <ul> <li> <p> Vendor product configured for the add-on</p> </li> <li> <p> Splunk instance with add-on installed</p> </li> <li> <p> The setup is manually tested</p> </li> </ul> </li> <li> <p> openapi.json saved to developer workstation</p> </li> <li> <p> docker installed and started</p> </li> </ul> <p>Open configuration and download it to <code>splunk-example-ta-test/</code> directory using OpenAPI.json button.</p> Click to check where we are with the prerequisites <ul> <li> <p> Prepared basic setup for the add-on</p> <ul> <li> <p> Vendor product configured for the add-on</p> </li> <li> <p> Splunk instance with add-on installed</p> </li> <li> <p> The setup is manually tested</p> </li> </ul> </li> <li> <p> openapi.json saved to developer workstation</p> </li> <li> <p> docker installed and started</p> </li> </ul> <p>You\u2019ve got openapi.json that will be used in following steps. Moreover, you confirmed that you\u2019ve got all you need to create necessary environment for development. You can delete docker containers <pre><code>docker rm -f server-example-ta splunk-example-ta\n</code></pre> and recreate the environment <pre><code>./scripts/run_locally.sh\n</code></pre></p> <p>Note: The containers recreation is just one of a few options to prepare the environment for development. If you are not interested in having clean instance, you may consider:</p> <ul> <li> <p>inputs deactivation only</p> </li> <li> <p>inputs and configuration deletion</p> </li> <li> <p>etc.</p> </li> </ul>"},{"location":"hello_world/#init","title":"init","text":"<p>Open <code>splunk-example-ta-test/</code> directory in terminal. There should be openapi.json file downloaded as a part of satisfying prerequisities.</p> <p>Install <code>addonfactory-ucc-test</code> and make sure it is installed <pre><code>pip install splunk-add-on-ucc-modinput-test\nucc-test-modinput --version\n</code></pre></p> <p>Initialize modinput tests <pre><code>ucc-test-modinput init --openapi-json openapi.json\n</code></pre> Your subdirectories structure should look like <pre><code>.\n\u251c\u2500\u2500 swagger_client\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 api\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 models\n\u2514\u2500\u2500 tests\n    \u2514\u2500\u2500 ucc_modinput_functional\n        \u251c\u2500\u2500 splunk\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 client\n        \u2514\u2500\u2500 vendor\n            \u2514\u2500\u2500 client\n</code></pre></p> <p>Hint: If you use version control system such as git, you don\u2019t want to keep there <code>swagger_client/</code> that will be generated for you from <code>openapi.json</code> by <code>ucc-test-modinput</code>.</p> <p>Set environment variables for your Splunk instance. <pre><code>export MODINPUT_TEST_SPLUNK_HOST=localhost\nexport MODINPUT_TEST_SPLUNK_PORT=8089\nexport MODINPUT_TEST_SPLUNK_USERNAME=admin\nexport MODINPUT_TEST_SPLUNK_PASSWORD_BASE64=$(ucc-test-modinput base64encode -s 'Chang3d!')\n</code></pre></p> <p>Run few auto-generated tests  <pre><code>pytest tests/ucc_modinput_functional\n</code></pre> You should be informed about 3 passed tests.</p> <p>We will be interested in <code>splunk-example-ta-test/tests/ucc_modinput_functional/</code> when working on following points of the instruction <pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 defaults.py\n\u251c\u2500\u2500 splunk\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 client\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 _managed_client.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 client.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 configuration.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 forges.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 probes.py\n\u251c\u2500\u2500 test_settings.py\n\u2514\u2500\u2500 vendor\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 client\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 client.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 configuration.py\n    \u251c\u2500\u2500 forges.py\n    \u2514\u2500\u2500 probes.py\n</code></pre></p>"},{"location":"hello_world/#test_ta_logging-your-first-test","title":"test_ta_logging - your first test","text":"<p>We want to have log level set to DEBUG for all of the tests we will write.</p> <p>As the log level will be so common, we can add it to <code>defaults.py</code> <pre><code>TA_LOG_LEVEL_FOR_TESTS = \"DEBUG\"\n</code></pre></p> <p>We will create appropriate test, to make sure log level is changed to DEBUG.</p> <p>Let\u2019s have dedicated file to test modifications in addon configuration - <code>test_configuration.py</code>. Move piece of code for log level from <code>test_settings.py</code> to <code>test_configuration.py</code> and adopt.</p> <p>The code we are to use from <code>test_settings.py</code> <pre><code>@attach(forge(set_loglevel, loglevel=\"CRITICAL\", probe=wait_for_loglevel))\ndef test_valid_loglevel(splunk_client: SplunkClient, wait_for_loglevel: bool) -&gt; None:\n    assert wait_for_loglevel is True\n</code></pre></p> <p>The code how it should look like in <code>test_configuration.py</code> <pre><code>@bootstrap(\n    forge(\n        set_loglevel,\n        loglevel=defaults.TA_LOG_LEVEL_FOR_TESTS,\n        probe=wait_for_loglevel,\n    )\n)\ndef test_ta_logging(splunk_client: SplunkClient) -&gt; None:\n    assert (\n        splunk_client.get_settings_logging()[\"loglevel\"]\n        == defaults.TA_LOG_LEVEL_FOR_TESTS\n    )\n</code></pre> You need add few imports to make it works <pre><code>from splunk_add_on_ucc_modinput_test.functional.decorators import (\n    bootstrap,\n    forge,\n)\nfrom tests.ucc_modinput_functional.splunk.forges import (\n    set_loglevel,\n)\nfrom tests.ucc_modinput_functional.splunk.probes import (\n    wait_for_loglevel,\n)\nfrom tests.ucc_modinput_functional.splunk.client import SplunkClient\nfrom tests.ucc_modinput_functional import defaults\n</code></pre> We are ready to run our first test. Remember to do it from <code>splunk-example-ta-test/</code> <pre><code>pytest -v tests/ucc_modinput_functional/test_configuration.py\n</code></pre> You should see <pre><code>tests/ucc_modinput_functional/test_configuration.py::test_ta_logging PASSED [100%]\n</code></pre> Hint: Save modifications to your version control system</p>"},{"location":"hello_world/#test_accounts-first-this-addon-specific-test","title":"test_accounts - first, this addon-specific, test","text":"<p>We want to make sure account is created in addon configuration.</p> <p>Account configuration requires server API key. That is configuration relevant to server-example-ta - vendor product. API key is a credential. We would like to keep it as non-plain text environment variables: <pre><code>export MODINPUT_TEST_EXAMPLE_API_KEY_BASE64=$(ucc-test-modinput base64encode -s 'super-secret-api-token')\n</code></pre></p> <p>We need to document that for whoever will use our test. Open <code>splunk-example-ta-test/tests/ucc_modinput_functional/README.md</code> and add relevant information there. <pre><code>Alongside with environment variables for Splunk, export API key for server-example-ta:\n\n    ```console\n    export MODINPUT_TEST_EXAMPLE_API_KEY_BASE64=$(ucc-test-modinput base64encode -s 'super-secret-api-token')\n    ```\n</code></pre> Once we can be sure the variable is defined, we want to read it. Open <code>splunk-example-ta-test/tests/ucc_modinput_functional/vendor/client/configuration.py</code>, make sure the key is read from the variable and expose for use: <pre><code>from splunk_add_on_ucc_modinput_test.common import utils\n\nclass Configuration(VendorConfigurationBase):\n    def customize_configuration(self) -&gt; None:\n        self._api_key = utils.get_from_environment_variable(\n            \"MODINPUT_TEST_EXAMPLE_API_KEY_BASE64\",\n            string_function=utils.Base64.decode,\n        )\n\n    @property\n    def api_key(self) -&gt; Optional[str]:\n        return self._api_key\n</code></pre> Note: Remember to <code>from typing import Optional</code></p> <p>We will need to create an account for testing purposes. The framework provides generic methods for this, so  search for <code>create_account</code> in <code>splunk-example-ta-test/tests/ucc_modinput_functional/splunk/client/_managed_client.py</code>. </p> <p>You were already able to see (by <code>test_ta_logging</code> example) that test function is decorated with forge functions. Let\u2019s create one for the account in <code>splunk-example-ta-test/tests/ucc_modinput_functional/splunk/forges.py</code> <pre><code>def account(\n    splunk_client: SplunkClient,\n    vendor_client: VendorClient,\n) -&gt; Generator[Dict[str, str], None, None]:\n    account_config = {\n        \"name\": f\"ExampleAccount_{utils.Common().sufix}\",\n        \"api_key\": vendor_client.config.api_key,\n    }\n    splunk_client.create_account(**account_config)\n    yield dict(\n        account_config_name=account_config[\"name\"]\n    )\n</code></pre> Note: Remember to <code>from tests.ucc_modinput_functional.vendor.client import VendorClient</code></p> <p>We\u2019ve got all of the blocks ready now to build our test function. Open <code>splunk-example-ta-test/tests/ucc_modinput_functional/test_configuration.py</code> <pre><code>@bootstrap(\n    forge(\n        set_loglevel,\n        loglevel=defaults.TA_LOG_LEVEL_FOR_TESTS,\n        probe=wait_for_loglevel,\n    ),\n    forge(account),\n)\ndef test_accounts(\n    splunk_client: SplunkClient,\n    account_config_name: str,\n) -&gt; None:\n    actual_account = splunk_client.get_account(account_config_name)\n    assert actual_account is not None\n</code></pre> Note: Remember to <code>from tests.ucc_modinput_functional.splunk.forges import account</code></p> <p>We are ready to run test_accounts: <pre><code>pytest -v tests/ucc_modinput_functional/test_configuration.py::test_accounts\n</code></pre> You should get <pre><code>tests/ucc_modinput_functional/test_configuration.py::test_accounts PASSED\n</code></pre> Hint: Save modifications to your version control system</p>"},{"location":"hello_world/#test_inputs-to-make-sure-data-is-comming","title":"test_inputs to make sure data is comming","text":"<p>We want to make sure input is created, data is ingested and input is deactivated. Goal is to have it available for troubleshooting if needed but we don\u2019t want to keep the Splunk instance too busy with inputs active once events necessary for tests were already ingested.</p> <p>Let\u2019s find relevant methods to create and deactivate inputs in <code>splunk-example-ta-test/tests/ucc_modinput_functional/splunk/client/_managed_client.py</code> \u2013 <code>create_example</code> and <code>update_example</code>.</p> <p>When creating input, we\u2019ll use some default value for interval. Add following to <code>defaults.py</code>: <pre><code>INPUT_INTERVAL = 60\n</code></pre></p> <p>In case of inputs, we want to be sure data is coming to specific index, source related to just created input and after the input gets created.</p> <p>Whatever needs to happen before test execution, needs to be added before <code>yield</code> (test setup). <code>yield</code>ed are values used for tests, other forges, probes, etc. What happens after, needs to be added after <code>yield</code> (teardown).</p> <p>Let\u2019s add <code>example_input</code> forge containing all the knowledge documented above to <code>tests/ucc_modinput_functional/splunk/forges.py</code>: <pre><code>def example_input(\n    splunk_client: SplunkClient,\n    *,\n    account_config_name: str,   # was defined in account forge\n) -&gt; Generator[Dict[str, str], None, None]:\n    name = f\"ExampleInput_{utils.Common().sufix}\"\n    index = splunk_client.splunk_configuration.dedicated_index.name\n    start_time = utils.get_epoch_timestamp()\n    splunk_client.create_example(name, defaults.INPUT_INTERVAL, index, account_config_name)\n    input_spl = (\n        f'search index={index} source=\"example://{name}\" '\n        f\"| where _time&gt;{start_time}\"\n    )\n    yield dict(input_spl_name=input_spl)\n    splunk_client.update_example(name, disabled=True)\n</code></pre> Note: You need to add two imports: <pre><code>from splunk_add_on_ucc_modinput_test.common import utils\nfrom tests.ucc_modinput_functional import defaults\n</code></pre></p> <p>Once some configuration is added, modified or deleted and effect is not immediate, we use probes to wait with further steps until effects occur. Open <code>tests/ucc_modinput_functional/splunk/probes.py</code> and add <code>events_ingested</code>: <pre><code>def events_ingested(\n    splunk_client: SplunkClient, input_spl_name: str, probes_wait_time: int = 10\n) -&gt; Generator[int, None, None]:\n    while True:\n        search = splunk_client.search(searchquery=input_spl_name)\n        if search.result_count != 0:\n            break\n        yield probes_wait_time\n</code></pre></p> <p>Input configuration requires account configuration that was tested in previous section. Moreover, just like for all the other tests, we want to make sure log level is set to default.</p> <p>Let\u2019s have dedicated test file for inputs - <code>test_inputs.py</code> in <code>tests/ucc_modinput_functional/</code> with <code>test_input</code>: <pre><code>@bootstrap(\n    forge(\n        set_loglevel,\n        loglevel=defaults.TA_LOG_LEVEL_FOR_TESTS,\n        probe=wait_for_loglevel,\n    ),\n    forge(account),\n    forge(\n        example_input,\n        probe=events_ingested,\n    )\n)\ndef test_inputs(splunk_client: SplunkClient, input_spl_name: str) -&gt; None:\n    search_result_details = splunk_client.search(searchquery=input_spl_name)\n    assert (\n        search_result_details.result_count != 0\n    ), f\"Following query returned 0 events: {input_spl_name}\"\n\n    utils.logger.info(\n        \"test_inputs_loginhistory_clone done at \"\n        + utils.convert_to_utc(utils.get_epoch_timestamp())\n    )\n</code></pre> Note: You need to have imports you have in <code>test_configuration.py</code> and few new:</p> <ul> <li> <p><code>from splunk_add_on_ucc_modinput_test.common import utils</code></p> </li> <li> <p><code>example_input</code> add to imports from <code>tests.ucc_modinput_functional.splunk.forges</code></p> </li> <li> <p><code>events_ingested</code> from <code>tests.ucc_modinput_functional.splunk.probes</code></p> </li> </ul> <p>Run test_inputs: <pre><code>pytest -v tests/ucc_modinput_functional/test_inputs.py::test_inputs\n</code></pre> You should get <pre><code>tests/ucc_modinput_functional/test_configuration.py::test_accounts PASSED\n</code></pre> Hint: Remember about saving to version control system</p>"},{"location":"hello_world/#want-to-see-more-examples","title":"\u2026 want to see more examples?","text":"<p>Check the tests implementation for Example TA.</p>"},{"location":"hello_world/#troubleshooting","title":"troubleshooting","text":"<ul> <li> <p>This tutorial uses splunk-example-ta, so consider checking documentation for this project when facing any unexpected error. </p> </li> <li> <p>In case of <code>npm error code E401 npm error Incorrect or missing password. ...</code> error, please move your <code>~/.npmrc</code> file to <code>~/.npmrc.backup</code>: <code>mv ~/.npmrc ~/.npmrc.backup</code></p> </li> </ul>"},{"location":"test_scenarios/","title":"Test scenarios","text":"<p>General test cases are described. There are scenarios for each and relevant concepts that should be used.</p> <p>Note: all <code>forge</code> tasks should be treated as <code>bootstrap</code> unless explicitly defined as <code>attach</code></p> <p>Note: if <code>forge</code> term is used, that generally refers to setup step unless explicitly defined as teardown</p>"},{"location":"test_scenarios/#basic-scenario","title":"Basic scenario","text":"<p>We want to ingest some general events for few inputs and vendor product is talkative enough to expose the event within seconds or few minutes.</p> <ol> <li> <p>Increase log level to DEBUG (forge)</p> </li> <li> <p>Create configuration (forge; yield configuration name - will be used for input)</p> </li> <li> <p>Create inputs that depend on just created configuration (forge with probe - will be used to wait; yield SPL query that should contain at least index, source and start time information)</p> </li> <li> <p>Wait till events are indexed (probe; use SPL query)</p> </li> <li> <p>Test - assert event/actual values are as expected (use SPL query; assert returned values against expected values)</p> </li> <li> <p>Disable inputs (forge teardown)</p> </li> <li> <p>Decrease log level to initial value (forge teardown)</p> </li> </ol>"},{"location":"test_scenarios/#isolate-data-in-indexes","title":"Isolate data in indexes","text":"<p>Note: this case is just an extension of Basic scenario and as such, just concepts that touches the differences will be described</p> <p>Specifics of an add-on considered in this scenario does not allow to distinguish which input was used to ingest specific event.</p> <p>Hint: When constructing tests for this kind of add-on, you want to have dedicated index for each input</p> <ol> <li> <p>Increase log level to DEBUG</p> </li> <li> <p>Create (<code>forges</code> as following can be done independently)</p> <ol> <li> <p>configuration</p> </li> <li> <p>indexes (forge per index; yield index name - will be used for input)</p> </li> </ol> </li> <li> <p>Create inputs with reference to just created configuration and indexes</p> </li> <li> <p>Wait</p> </li> <li> <p>Test</p> </li> <li> <p>Disable inputs</p> </li> <li> <p>Decrease log level</p> </li> </ol>"},{"location":"test_scenarios/#test-proxies","title":"Test proxies","text":"<p>Note: this case is just an extension of Basic scenario and as such, just concepts that touches the differences will be described</p> <p>We want to be sure the add-on can be configured to use proxy if needed.</p> <p>Hint: Proxy configuration is general for specific add-on, so if defined it will be used for all configuration entries as well as inputs. When constructing this kind of tests you want to isolate them that can be achieved by using <code>attach</code> decorator that would group following tasks</p> <ol> <li> <p>Increase log level to DEBUG </p> </li> <li> <p>Configure proxy or disabled if we want to test without proxy configured (<code>attache</code> to be sure all following forge tasks are in the context of this configuration)</p> <ol> <li> <p>Create configuration - we want to be sure proxy configuration is applied to it, especially if connection to vendor product is established to validate configuration corectness</p> </li> <li> <p>Create inputs</p> </li> <li> <p>Wait</p> </li> <li> <p>Test</p> </li> <li> <p>Disable inputs</p> </li> </ol> </li> <li> <p>Decrease log level</p> </li> </ol>"},{"location":"test_scenarios/#trigger-events-generation","title":"Trigger events generation","text":"<p>We want to ingest some general events for an input and vendor product needs to be triggered to generate the events first.</p> <ol> <li> <p>Increase log level to DEBUG (forge)</p> </li> <li> <p>Following steps can be executed independently, before relevant input is created (forges)</p> <ol> <li> <p>Create configuration (forge; yield configuration name - will be used for input)</p> </li> <li> <p>Trigger vendor product to generate event (forge; yield timestamp)</p> </li> </ol> </li> <li> <p>Create input that depend on just created configuration and timestamp (forge with probe - will be used to wait; yield SPL query that should contain at least index, source and start time information)</p> </li> <li> <p>Wait till events are indexed (probe; use SPL query)</p> </li> <li> <p>Test - assert event/actual values are as expected (use SPL query; assert returned values against expected values)</p> </li> <li> <p>Disable input (forge teardown)</p> </li> <li> <p>Decrease log level to initial value (forge teardown)</p> </li> </ol>"},{"location":"test_scenarios/#configure-vendor-product","title":"Configure vendor product","text":"<p>Note: this case is just an extension of triggering events generation and as such, just concepts that touches the differences will be described</p> <p>Vendor product needs to be configured before it can be triggered to generate the events. The vendor product configuration has to be roll back then.</p> <ol> <li> <p>Increase log level to DEBUG</p> </li> <li> <p>Configure vendor product (forge; yield configuration name - it can be used later for configuration or input and the configuration teardown)</p> </li> <li> <p>Before input is created</p> <ol> <li> <p>Create configuration</p> </li> <li> <p>Trigger vendor product to generate event</p> </li> </ol> </li> <li> <p>Create input</p> </li> <li> <p>Wait</p> </li> <li> <p>Test</p> </li> <li> <p>Disable input</p> </li> <li> <p>Delete vendor product configuration (forge teardown)</p> </li> <li> <p>Decrease log level</p> </li> </ol>"},{"location":"test_scenarios/#eliminate-dangling-resources","title":"Eliminate dangling resources","text":"<p>Note: this case is just an extension of configuring vendor product and as such concepts that touches the differences will be described</p> <p>It happens that teardown is not reached in the tests. There can be number of reasons - eg. developer interupts tests execution before teardown is reached.</p> <p>We have to maintain hygiene in vendor product configuration to eliminate dangling resources.</p> <p>Hint: We want to be able to distinguish configuration created for our tests from: 1. configuration used for other purposes, 2. configuration created for our tests but other time, 3. configuration created for our tests but by other test run - that may be a case in CI</p> <p>All the steps are the same as for configuring vendor product, beside implementation of:</p> <ol> <li> <p>Configure vendor product:</p> <ol> <li> <p>Check list of configuration items and filter by names. Process only if name: 1. matches predefined pattern for the tests, 2. timestamp shows the configuration is older than predefined threshold. Delete the resources.</p> </li> <li> <p>When creating the configuration, make sure its name 1. matches predefined pattern, 2. contains timestamp and 3. contains test id.</p> </li> </ol> </li> </ol>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#ci-issues","title":"CI issues","text":"<ol> <li> <p>While installing the framework from Git instead of PyPI, you may encounter the following error:     <pre><code>HangupExceptionThe remote server unexpectedly closed the connection\n....\nThe following error occurred when trying to handle this error:\nHangupException\n\ngit@github.com: Permission denied (publickey)\n....\n</code></pre></p> <p>To resolve that, please install the package using PyPI. If that is not possible, and you use <code>addonfactory-workflow-addon-release</code>, please make sure you\u2019re using at least <code>v4.19</code> version. 2. If you encounter the following error: <pre><code>Existing splunk client (tests/ucc_modinput_functional/splunk/client/_managed_client.py) is outdated. Use --force-splunk-client-overwritten to overwrite it or --skip-splunk-client-check if you want to post.\n</code></pre> Please make sure to synchronize your output folder with the latest TA dev state and regenerate <code>_managed_client.py</code> by running </p> <p><pre><code>ucc-gen build \nucc-test-modinput gen --force-splunk-client-overwritten\n</code></pre>    and add the <code>_managed_client.py</code> file to your commit.</p> </li> </ol>"},{"location":"ucc-test-modinput_cli_tool/","title":"ucc-test-modinput CLI tool","text":"<p>ucc-test-modinput CLI is a supporting tool.</p> <p>It comes with following arguments:</p> <ul> <li> <p><code>--help</code> or <code>-h</code> ; shows help message and exits ; you can use it for arguments as well - eg. <code>ucc-test-modinput base64encode -h</code> will show help message for <code>base64encode</code></p> </li> <li> <p><code>--version</code> - shows program\u2019s version number and exit </p> </li> <li> <p><code>base64encode</code> - converts complex string (due to special characters or structure) to base64 string</p> <ul> <li> <p><code>--string</code> or <code>-s</code> ; <code>-s [string you want to encode]</code> - eg. <code>base64encode -s ThisIsMyPassword</code></p> </li> <li> <p><code>--file</code> or <code>-f</code> ; <code>-f [text file path; string from the file will be encoded]</code> - eg. <code>ucc-test-modinput base64encode -f ~/client_secret.json</code></p> </li> </ul> </li> <li> <p><code>base64decode -s [string you want to decode]</code> - eg. <code>ucc-test-modinput base64decode -s VGghczEkTXlQQHNzdzByZA==</code></p> </li> <li> <p><code>gen</code> - does two things: 1. creates add-on SDK from given openapi.json, 2. creates splunk client module and checks if existing (<code>ucc_modinput_functional/splunk/client/client.py</code>) is the same</p> <ul> <li> <p><code>--openapi-json</code> or <code>-o</code> ; <code>-o [path to openapi.json / source file ]</code> - default value is <code>output/*/appserver/static/openapi.json</code> ; refer to UCC documentation to learn more where you can find this document</p> </li> <li> <p><code>--client-code</code> or <code>-c</code> ; <code>-c [path to client code / target directory]</code> - default value is set to repo root directory ; this is where <code>swagger_client</code> directory will be saved or overwriten if some exists already. The directory contains client code for TA REST API and <code>swagger_client/README.md</code> file that documents the client API</p> </li> <li> <p><code>--tmp</code> or <code>-t</code> ; <code>-t [path to directory where temporary files are stored]</code> - default value is set to <code>/modinput/</code> subdirectory of directory used for temporary files</p> </li> <li> <p><code>--platform</code> or <code>-p</code> - not used by default ; <code>--platform</code> flag that can be used to run swaggerapi/swagger-codegen-cli-v3 docker image</p> </li> <li> <p><code>--skip-splunk-client-check</code> - exisitng splunk client will not be checked aginst consistency with swagger client that may lead to inconsistent state of splunk client; this is <code>gen</code> specific flag and does not exists for <code>init</code></p> </li> <li> <p><code>--force-splunk-client-overwritten</code> - existing splunk client will be backup and overwritten by new one; this is <code>gen</code> specific flag and does not exists for <code>init</code></p> </li> </ul> </li> <li> <p><code>init</code> - initialize modinput tests (you can read more on that here) and runs <code>gen</code> to have add-on SDK created ; none additional argument is required for the initialization step, so argument list is as for <code>gen</code> (excluding <code>--skip-splunk-client-check</code> and <code>--force-splunk-client-overwritten</code>)</p> </li> </ul>"},{"location":"when_you_write_your_tests/","title":"When you write your tests","text":"<p>Running <code>ucc-test-modinput init</code> provides a starting point for further development.</p> <p>Start with the basic case even if you need to cover more complex test cases. This will allow you to ensure there is access from the development environment to the vendor product.</p> <p>This paragraph contains hints that should be useful for your test development.</p> <p>Keep checking the example for implementation details.</p>"},{"location":"when_you_write_your_tests/#vendor-product","title":"Vendor product","text":""},{"location":"when_you_write_your_tests/#general-hints","title":"General hints","text":"<p>Vendor product specific code is entirely in developer\u2019s hands. There are however some hints you may find useful:</p> <ol> <li> <p>Consult Product Documentation:    Begin with the official product documentation. These resources often include code samples and integration guides that can be directly applied to your tests, saving development time and effort.</p> </li> <li> <p>Explore Official Repositories:    Check vendor official repositories (eg. GitHub). These repositories might contain supporting libraries or example code that can aid in developing integrations.</p> </li> <li> <p>Leverage Package Indexes:    Utilize PyPI.org or equivalent package indexes for discovering SDKs and libraries that are specific to the vendor products. These SDKs can simplify the integration process and ensure compatibility.</p> </li> <li> <p>Utilize OpenAPI Specifications:    If available, use OpenAPI or equivalent specifications to create or generate client libraries for the vendor products. This can facilitate a more streamlined and automated integration process.</p> </li> <li> <p>Engage with Developer Communities:    Platforms like Reddit and StackOverflow are valuable for community support. You can find discussions, troubleshooting tips, and shared experiences related to integrating vendor products.</p> </li> <li> <p>Consult AI Tools:    Consider using AI services to assist with coding, integration challenges, or generating documentation. These tools can provide insights or generate code snippets that may enhance your framework.</p> </li> </ol>"},{"location":"when_you_write_your_tests/#framework-specific-hints","title":"Framework specific hints","text":"<p>It is highly recommended to stay consistent with Splunk specific code to have internally consistent and easier to maintain tests. To achieve it, consider following hints:</p> <ol> <li> <p>Follow <code>vendor/</code> directory structure as described in the example TA</p> </li> <li> <p>Store credentials in environment variables and use <code>get_from_environment_variable</code> from <code>splunk_add_on_ucc_modinput_test.common.utils</code> to read the credentials</p> <ol> <li>if environment variable is optional, use <code>is_optional=True</code> parameter - eg.:</li> </ol> <p><pre><code>self.username = utils.get_from_environment_variable(\"MODINPUT_TEST_FOOBAR_USERNAME\", is_optional=True)\n</code></pre> <code>None</code> will be assigned to <code>self.username</code> in example as above</p> <ol> <li> <p>if environent variable should be encoded, use relevant sufix to emphasize a fact it was done and use <code>string_function</code> parameter when calling <code>get_from_environment_variable</code> function - eg.: <pre><code>self.token = utils.get_from_environment_variable(\"MODINPUT_TEST_FOOBAR_TOKEN_BASE64\", string_function=utils.Base64.decode)\n</code></pre> in example as above:</p> <ul> <li> <p><code>_BASE64</code> suffix is used to emphasize the password value should be base64 encoded</p> </li> <li> <p><code>string_function</code> is pointing to callable object that will do string transformation.</p> </li> </ul> </li> </ol> </li> </ol>"},{"location":"when_you_write_your_tests/#splunk","title":"Splunk","text":"<p>Proper spl query construction is crucial for proper tests results and performance.</p> <p>The spl query format looks like below:</p> <p><pre><code>search index={index_name} source={source_name_containing_input_name} {raw_event_specific_string} | where _time&gt;{start_time}\n</code></pre> where:</p> <ul> <li> <p>index_name can be:</p> <ul> <li> <p>default - assigned to <code>splunk_client.splunk_configuration.dedicated_index.name</code></p> </li> <li> <p>dedicated - check data isolation principle or relevant test scenario for more</p> </li> </ul> </li> <li> <p>source_name_containing_input_name highly depends on add-on implementation; eg. for example add-on source part of the spl is defined as <code>source=\"example://{name}\"</code> (where <code>name</code> is for input name)</p> </li> <li> <p>raw_event_specific_string that can be skipped, if other values are sufficient, one or many strings that define uniquely raw event we are interested in</p> </li> <li> <p>start_time - epoch timestamp should be used, however where timestamp is collected should be pick with special care. Check test scenarios to understand what potential options you\u2019ve got. Timestamp of begginig of tests can be used as default: <code>start_time = utils.get_epoch_timestamp()</code></p> </li> </ul>"},{"location":"when_you_write_your_tests/#when-your-tests-are-ready","title":"When your tests are ready","text":"<ol> <li> <p>Export environment variables for Splunk and for vendor</p> </li> <li> <p>Run tests: <code>pytest tests/ucc_modinput_functional/</code> (and fix tests if needed)</p> </li> <li> <p>Document add-on specific information related to the functional tests in <code>tests/ucc_modinput_functional/README.md</code>. Particularly - how vendor product should be prepared (or reference to relevant documentation) as well as what vendor and test specific environment variables should be exported</p> </li> <li> <p>Commit and push your modifications to code repository. Ignore <code>output/</code> and <code>swagger_client/</code> directories that are generated by <code>ucc-gen gen</code> and <code>ucc-test-modinput gen</code> respectively.</p> </li> </ol>"}]}